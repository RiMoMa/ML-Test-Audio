{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Test Part 1 - Data Preprocessing",
   "id": "d970fbedc85e6ca1"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "from pathlib import Path\n",
    "import re\n",
    "from typing import List, Tuple, Dict, Optional\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import urllib.request\n",
    "import zipfile\n",
    "import shutil\n",
    "import os\n"
   ],
   "id": "d3c5e62c2df0cc0e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "# ----------------------------\n",
    "# 0) Config\n",
    "# ----------------------------\n",
    "DATA_URL = \"https://prod-dcd-datasets-cache-zipfiles.s3.eu-west-1.amazonaws.com/jwyy9np4gv-3.zip\" #download dataset if is required\n",
    "\n",
    "DATA_DIR = Path(\"./Audio Files\")  # <-- CHANGE THIS\n",
    "OUTPUT_DIR = Path(\"./outputs_part1_clean\")\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)"
   ],
   "id": "1fd2114db7abec69",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Part 1 — Ingest + Exploratory Data Analysis (EDA)\n",
    "\n",
    "This section focuses on robust dataset ingestion, parsing, and initial exploratory analysis.\n",
    "\n",
    "**Steps performed:**\n",
    "- **Dataset download & extraction**  \n",
    "  - Download outer ZIP and recursively extract nested archives.  \n",
    "  - Verify presence of `.wav` audio files.  \n",
    "\n",
    "- **Location parsing**  \n",
    "  - Normalize anatomical recording location (Plane–Side–Level).  \n",
    "  - Handle irregular tokenization (3-letter glued codes, 4-token cases).  \n",
    "  - Validate and audit parsing outcomes.  \n",
    "\n",
    "- **Sound types & diagnosis normalization**  \n",
    "  - Standardize sound codes/words (e.g., *I, E, W, C, N*).  \n",
    "  - Map diagnoses to canonical labels (e.g., Asthma, COPD, Pneumonia).  \n",
    "  - Split mixed header tokens into sound types vs. diagnoses.  \n",
    "\n",
    "- **Filename parsing**  \n",
    "  - Extract metadata: patient ID, filter (Bell/Diaphragm/Extended), location, age, gender.  \n",
    "  - Build structured records combining parsed attributes.  \n",
    "\n",
    "- **Dataset scanning**  \n",
    "  - Traverse all `.wav` files and build wide/long DataFrames.  \n",
    "  - Explode multi-label fields (diagnosis, sound type) for analysis.  \n",
    "\n",
    "- **Frequency tables & simple plots**  \n",
    "  - Compute counts by filter, location, gender, diagnosis, sound type.  \n",
    "  - Generate bar charts and boxplots (age vs. diagnosis).  \n"
   ],
   "id": "38bc41859e97e3d0"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [
    "# ============================\n",
    "# Part 1 — Ingest + EDA (clean, robust, audited)\n",
    "# ============================\n",
    "\n",
    "\n",
    "\n",
    "    # ============================\n",
    "    # 0) Download and extract dataset \n",
    "    # ============================\n",
    "def download_and_extract(url: str = DATA_URL,\n",
    "                         work_dir: Path = Path(\"./data_raw\"),\n",
    "                         extract_dirname: str = \"jwyy9np4gv-3\",\n",
    "                         force_redownload: bool = False) -> Path:\n",
    "    \"\"\"\n",
    "    Download the dataset ZIP and extract all nested zips (Audio Files, Stethoscope Files).\n",
    "    - After execution you should have all .wav files available under work_dir/extract_dirname.\n",
    "    - Returns the path to the extracted main folder.\n",
    "    \"\"\"\n",
    "    work_dir = Path(work_dir)\n",
    "    work_dir.mkdir(parents=True, exist_ok=True)\n",
    "    zip_path = work_dir / \"lung_sounds.zip\"\n",
    "    extract_dir = work_dir / extract_dirname\n",
    "\n",
    "    # If already extracted with WAVs, skip\n",
    "    if extract_dir.exists() and any(extract_dir.rglob(\"*.wav\")) and not force_redownload:\n",
    "        print(f\"[OK] Dataset already available at: {extract_dir}\")\n",
    "        return extract_dir\n",
    "\n",
    "    # Clean up if forced\n",
    "    if force_redownload:\n",
    "        if zip_path.exists():\n",
    "            zip_path.unlink()\n",
    "        if extract_dir.exists():\n",
    "            shutil.rmtree(extract_dir, ignore_errors=True)\n",
    "\n",
    "    # Download the outer ZIP\n",
    "    if not zip_path.exists():\n",
    "        print(f\"[...] Downloading dataset from:\\n{url}\")\n",
    "        urllib.request.urlretrieve(url, zip_path)\n",
    "        print(f\"[OK] ZIP saved at: {zip_path}\")\n",
    "\n",
    "    # Extract outer ZIP\n",
    "    print(f\"[...] Extracting outer ZIP to: {extract_dir}\")\n",
    "    extract_dir.mkdir(parents=True, exist_ok=True)\n",
    "    with zipfile.ZipFile(zip_path, \"r\") as zf:\n",
    "        zf.extractall(extract_dir)\n",
    "\n",
    "    # Now extract inner zips if they exist\n",
    "    for inner_zip in [\"Audio Files.zip\", \"Stethoscope Files.zip\"]:\n",
    "        inner_path = extract_dir / inner_zip\n",
    "        if inner_path.exists():\n",
    "            inner_target = extract_dir / inner_zip.replace(\".zip\", \"\")\n",
    "            print(f\"[...] Extracting nested zip: {inner_zip}\")\n",
    "            inner_target.mkdir(parents=True, exist_ok=True)\n",
    "            with zipfile.ZipFile(inner_path, \"r\") as zf:\n",
    "                zf.extractall(inner_target)\n",
    "\n",
    "    # Verify WAV files\n",
    "    wavs = list(extract_dir.rglob(\"*.wav\"))\n",
    "    print(f\"[OK] WAV files found: {len(wavs)}\")\n",
    "    return extract_dir\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# 1) Location parsing (official P/A, L/R, U/M/L) with 4-token handling\n",
    "# ----------------------------\n",
    "PLANE_FULL = {\"P\": \"Posterior\", \"A\": \"Anterior\"}\n",
    "SIDE_FULL  = {\"L\": \"Left\", \"R\": \"Right\"}\n",
    "LEVEL_FULL = {\"U\": \"Upper\", \"M\": \"Middle\", \"L\": \"Lower\"}\n",
    "\n",
    "def _loc_tokenize(raw: str):\n",
    "    \"\"\"Tokenize location: uppercase, remove commas/tabs, split on whitespace; handle glued 3-char like 'PRU'.\"\"\"\n",
    "    s = re.sub(r\"[,+\\t]+\", \" \", str(raw).strip().upper())\n",
    "    toks = [t for t in re.split(r\"\\s+\", s) if t]\n",
    "    if len(toks) == 1 and len(toks[0]) == 3:  # e.g., 'PRU'\n",
    "        toks = list(toks[0])\n",
    "    return toks\n",
    "\n",
    "def _explode_letters(tokens: List[str]):\n",
    "    \"\"\"If any token has multiple letters and no spaces, split into single letters (e.g., 'LR' -> ['L','R']).\"\"\"\n",
    "    out = []\n",
    "    for t in tokens:\n",
    "        t = t.strip().upper()\n",
    "        if not t:\n",
    "            continue\n",
    "        if len(t) > 1 and \" \" not in t:\n",
    "            out.extend(list(t))\n",
    "        else:\n",
    "            out.append(t)\n",
    "    return out\n",
    "\n",
    "def parse_location_official(raw: Optional[str]):\n",
    "    out = {\n",
    "        \"location_raw\": raw,\n",
    "        \"location_norm\": None,\n",
    "        \"location_valid\": False,\n",
    "        \"location_issue\": None,\n",
    "        \"location_tokens_count\": None,\n",
    "        \"location_had_4_tokens\": False,\n",
    "        \"plane_tok\": None,\n",
    "        \"side_tok\": None,\n",
    "        \"level_tok\": None,\n",
    "    }\n",
    "    if raw is None or str(raw).strip() == \"\":\n",
    "        out[\"location_issue\"] = \"missing location\"\n",
    "        return out\n",
    "\n",
    "    # --- tokenize & plane ---\n",
    "    s = re.sub(r\"[,+\\t]+\", \" \", str(raw).strip().upper())\n",
    "    toks = [t for t in re.split(r\"\\s+\", s) if t]\n",
    "    if len(toks) == 1 and len(toks[0]) == 3:  # e.g. 'PRL'\n",
    "        toks = list(toks[0])\n",
    "    out[\"location_tokens_count\"] = len(toks)\n",
    "\n",
    "    plane, rest = toks[0], toks[1:]\n",
    "    if plane not in PLANE_FULL:\n",
    "        if len(plane) >= 2 and plane[0] in PLANE_FULL:\n",
    "            chars = list(plane) + sum((list(t) for t in rest), [])\n",
    "            plane, rest = chars[0], chars[1:]\n",
    "        else:\n",
    "            out[\"location_issue\"] = f\"invalid plane '{plane}'\";  return out\n",
    "\n",
    "    # explode letters (e.g. 'LR' -> ['L','R'])\n",
    "    exploded = []\n",
    "    for t in rest:\n",
    "        t = t.strip().upper()\n",
    "        exploded.extend(list(t) if len(t) > 1 and \" \" not in t else [t])\n",
    "    rest = exploded\n",
    "    if not rest:\n",
    "        out.update({\"plane_tok\": plane, \"location_issue\": \"missing side and level\"})\n",
    "        return out\n",
    "\n",
    "    if len(rest) >= 4:\n",
    "        out[\"location_had_4_tokens\"] = True\n",
    "\n",
    "    # --- FIX: pick LEVEL first (rightmost U/M/L), then SIDE (rightmost L/R excluding level index) ---\n",
    "    level_idx = None\n",
    "    for i in range(len(rest) - 1, -1, -1):\n",
    "        if rest[i] in LEVEL_FULL:\n",
    "            level_idx = i\n",
    "            break\n",
    "\n",
    "    side_idx = None\n",
    "    for i in range(len(rest) - 1, -1, -1):\n",
    "        if i == level_idx:\n",
    "            continue\n",
    "        if rest[i] in SIDE_FULL:\n",
    "            side_idx = i\n",
    "            break\n",
    "\n",
    "    # handle issues\n",
    "    if level_idx is None and side_idx is None:\n",
    "        out.update({\"plane_tok\": plane, \"location_issue\": \"no side and no level found\"})\n",
    "        return out\n",
    "    if level_idx is None:\n",
    "        out.update({\"plane_tok\": plane, \"side_tok\": rest[side_idx], \"location_issue\": \"missing level\"})\n",
    "        return out\n",
    "    if side_idx is None:\n",
    "        out.update({\"plane_tok\": plane, \"level_tok\": rest[level_idx], \"location_issue\": \"missing side\"})\n",
    "        return out\n",
    "\n",
    "    plane_tok = plane\n",
    "    level_tok = rest[level_idx]   # chosen first\n",
    "    side_tok  = rest[side_idx]    # chosen excluding level index\n",
    "\n",
    "    # validate & build label\n",
    "    if plane_tok not in PLANE_FULL:\n",
    "        out[\"location_issue\"] = f\"invalid plane '{plane_tok}'\"; return out\n",
    "    if side_tok not in SIDE_FULL:\n",
    "        out[\"location_issue\"] = f\"invalid side '{side_tok}'\";  return out\n",
    "    if level_tok not in LEVEL_FULL:\n",
    "        out[\"location_issue\"] = f\"invalid level '{level_tok}'\"; return out\n",
    "\n",
    "    out.update({\n",
    "        \"plane_tok\": plane_tok,\n",
    "        \"side_tok\": side_tok,\n",
    "        \"level_tok\": level_tok,\n",
    "        \"location_norm\": f\"{PLANE_FULL[plane_tok]} {SIDE_FULL[side_tok]} {LEVEL_FULL[level_tok]}\",\n",
    "        \"location_valid\": True,\n",
    "        \"location_issue\": None,\n",
    "    })\n",
    "    return out\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# 2) Sound types + diagnosis normalization\n",
    "# ----------------------------\n",
    "SOUND_CODE_MAP = {\n",
    "    \"I\": \"Inspiratory\",\n",
    "    \"E\": \"Expiratory\",\n",
    "    \"W\": \"Wheezes\",\n",
    "    \"C\": \"Crackles\",\n",
    "    \"N\": \"Normal\",\n",
    "    \"B\": \"Bronchial\",\n",
    "}\n",
    "SOUND_WORD_MAP = {\n",
    "    \"inspiratory\": \"Inspiratory\",\n",
    "    \"expiratory\": \"Expiratory\",\n",
    "    \"wheeze\": \"Wheezes\",\n",
    "    \"wheezes\": \"Wheezes\",\n",
    "    \"crackle\": \"Crackles\",\n",
    "    \"crackles\": \"Crackles\",\n",
    "    \"crep\": \"Crackles\",\n",
    "    \"crepitations\": \"Crackles\",\n",
    "    \"normal\": \"Normal\",\n",
    "    \"bronchial\": \"Bronchial\",\n",
    "}\n",
    "def _sound_from_word(word: str):\n",
    "    \"\"\"Return standardized sound types for codes ('EW', 'I C') or words ('Crep', 'Bronchial').\"\"\"\n",
    "    if not word:\n",
    "        return []\n",
    "    w_clean = re.sub(r\"[^A-Za-z]\", \"\", word)\n",
    "    if not w_clean:\n",
    "        return []\n",
    "    up = w_clean.upper()\n",
    "    if up in SOUND_CODE_MAP:  # single letter\n",
    "        return [SOUND_CODE_MAP[up]]\n",
    "    if re.fullmatch(r\"[IEWCBN]+\", up):  # glued letters like 'IEW'\n",
    "        return [SOUND_CODE_MAP[ch] for ch in up]\n",
    "    lw = w_clean.lower()\n",
    "    if lw in SOUND_WORD_MAP:\n",
    "        return [SOUND_WORD_MAP[lw]]\n",
    "    return []\n",
    "\n",
    "# Diagnosis canonical map (extend as needed)\n",
    "DIAG_CANONICAL = {\n",
    "    \"n\": \"Normal\",\n",
    "    \"N\": \"Normal\",\n",
    "    \"normal\": \"Normal\",\n",
    "    \"asthma\": \"Asthma\",\n",
    "    \"copd\": \"COPD\",\n",
    "    \"pneumonia\": \"Pneumonia\",\n",
    "    \"heart failure\": \"Heart Failure\",\n",
    "    \"lung fibrosis\": \"Lung Fibrosis\",\n",
    "    \"bron\": \"BRON\",                      # keep BRON label, or map to 'Bronchiectasis/Bronchitis'\n",
    "    \"bronchitis\": \"BRON\",\n",
    "    \"bronchiectasis\": \"BRON\",\n",
    "    \"pleural effusion\": \"Pleural Effusion\",\n",
    "}\n",
    "\n",
    "def normalize_diagnosis_label(text: str):\n",
    "    \"\"\"Normalize diagnosis to a canonical label (case-insensitive).\"\"\"\n",
    "    t = re.sub(r\"\\s+\", \" \", str(text).strip())\n",
    "    key = t.lower()\n",
    "    if key in DIAG_CANONICAL:\n",
    "        return DIAG_CANONICAL[key]\n",
    "    # Try simple heuristics\n",
    "    if key.replace(\".\", \"\") == \"copd\":\n",
    "        return \"COPD\"\n",
    "    return t.title()  # default: title-case (e.g., 'Bronchial Asthma' -> 'Bronchial Asthma')\n",
    "\n",
    "def split_diag_and_sound(head_tokens: List[str]):\n",
    "    \"\"\"\n",
    "    HEAD may mix diagnoses and sound types. Split into:\n",
    "      - diagnoses (canonicalized)\n",
    "      - sound_types (standard labels)\n",
    "    Accept '+', 'and', '&' as connectors; preserve order and de-duplicate.\n",
    "    \"\"\"\n",
    "    diagnoses, sounds = [], []\n",
    "\n",
    "    def dedup(seq):\n",
    "        \"\"\"\n",
    "        Remove duplicates from a list while preserving the original order.\n",
    "        Example: [\"COPD\",\"Asthma\",\"COPD\"] -> [\"COPD\",\"Asthma\"]\n",
    "        \"\"\"\n",
    "        seen, out = set(), []\n",
    "        for x in seq:\n",
    "            if x not in seen:\n",
    "                seen.add(x); out.append(x)\n",
    "        return out\n",
    "\n",
    "    for token in head_tokens:\n",
    "        if not token or not token.strip():\n",
    "            continue\n",
    "        # Normalize connectors to '+', and split underscores as well\n",
    "        # Examples: \"N_N\" -> \"N+N\", \"Asthma and COPD\" -> \"Asthma+COPD\"\n",
    "        norm = token.strip()\n",
    "        norm = norm.replace(\"_\", \"+\")\n",
    "        norm = re.sub(r\"\\s+(?i:and)\\s+|&\", \"+\", norm)\n",
    "        parts = [p.strip() for p in norm.split(\"+\") if p.strip()]\n",
    "\n",
    "        for part in parts:\n",
    "            # Split remaining by whitespace into words/tokens\n",
    "            words = [w for w in re.split(r\"\\s+\", part) if w]\n",
    "            local_sounds, diag_words = [], []\n",
    "            for w in words:\n",
    "                s = _sound_from_word(w)  # returns ['Normal'] for 'N'/'normal', etc.\n",
    "                if s:\n",
    "                    local_sounds.extend(s)\n",
    "                else:\n",
    "                    diag_words.append(w)\n",
    "\n",
    "            if diag_words:\n",
    "                # Join words back and normalize the diagnosis label\n",
    "                diagnoses.append(normalize_diagnosis_label(\" \".join(diag_words)))\n",
    "            sounds.extend(local_sounds)\n",
    "\n",
    "    diagnoses = dedup(diagnoses)\n",
    "    sounds = dedup(sounds)\n",
    "\n",
    "    # Special rule: if header is only 'N' (or variations) -> set diagnosis='Normal'\n",
    "    if not diagnoses and any(st == \"Normal\" for st in sounds):\n",
    "        diagnoses = [\"Normal\"]\n",
    "\n",
    "    return diagnoses, sounds\n",
    "\n",
    "# ----------------------------\n",
    "# 3) Filename parser\n",
    "# ----------------------------\n",
    "FILTER_MAP = {\"BP\": \"Bell\", \"DP\": \"Diaphragm\", \"EP\": \"Extended\"}\n",
    "\n",
    "def parse_filename(fname: str):\n",
    "    \"\"\"\n",
    "    Expected: <FILTER><ID>_<HEAD>,<LOCATION>,<AGE>,<GENDER>.wav\n",
    "    - HEAD can mix diagnoses and sound-type hints (codes or words).\n",
    "    - LOCATION normalized via official Plane–Side–Level with 4-token handling.\n",
    "    \"\"\"\n",
    "    name = Path(fname).name\n",
    "    if not name.lower().endswith(\".wav\"):\n",
    "        return None\n",
    "    stem = name[:-4]\n",
    "    if \"_\" not in stem:\n",
    "        return None\n",
    "\n",
    "    left, right = stem.split(\"_\", 1)\n",
    "    m = re.match(r\"^(BP|DP|EP)(\\d+)$\", left, re.IGNORECASE)\n",
    "    if not m:\n",
    "        return None\n",
    "    filter_code = m.group(1).upper()\n",
    "    patient_id = m.group(2)\n",
    "\n",
    "    parts = [p.strip() for p in right.split(\",\")]\n",
    "    gender = parts[-1] if len(parts) >= 1 else None\n",
    "    age = None\n",
    "    if len(parts) >= 2:\n",
    "        try:\n",
    "            age = int(re.sub(r\"[^0-9]\", \"\", parts[-2]))\n",
    "        except Exception:\n",
    "            age = None\n",
    "    location_raw = parts[-3] if len(parts) >= 3 else None\n",
    "    head_tokens  = parts[:-3] if len(parts) >= 4 else (parts[:1] if len(parts) >= 1 else [])\n",
    "\n",
    "    # Normalize gender (M/F)\n",
    "    if isinstance(gender, str):\n",
    "        g = gender.strip().upper()\n",
    "        gender = g if g in {\"M\",\"F\"} else (g[:1] if g else None)\n",
    "\n",
    "    # Split HEAD into diagnoses and sound types\n",
    "    diagnoses, sound_types = split_diag_and_sound(head_tokens)\n",
    "\n",
    "    # Parse location with audits\n",
    "    loc = parse_location_official(location_raw)\n",
    "\n",
    "    return {\n",
    "        \"filename\": name,\n",
    "        \"filter_code\": filter_code,\n",
    "        \"filter\": FILTER_MAP.get(filter_code, filter_code),\n",
    "        \"patient_id\": patient_id,\n",
    "        \"diagnoses\": diagnoses,            # normalized diagnoses\n",
    "        \"sound_types\": sound_types,        # normalized sound types\n",
    "        \"location_raw\": loc[\"location_raw\"],\n",
    "        \"location_norm\": loc[\"location_norm\"],\n",
    "        \"location_valid\": loc[\"location_valid\"],\n",
    "        \"location_issue\": loc[\"location_issue\"],\n",
    "        \"location_tokens_count\": loc[\"location_tokens_count\"],\n",
    "        \"location_had_4_tokens\": loc[\"location_had_4_tokens\"],\n",
    "        \"plane_tok\": loc[\"plane_tok\"],\n",
    "        \"side_tok\":  loc[\"side_tok\"],\n",
    "        \"level_tok\": loc[\"level_tok\"],\n",
    "        \"age\": age,\n",
    "        \"gender\": gender,\n",
    "    }\n",
    "\n",
    "# ----------------------------\n",
    "# 4) Scan + DataFrames\n",
    "# ----------------------------\n",
    "def scan_dataset(data_dir: Path):\n",
    "    \"\"\"Return df_wide, df_long_diag (explode), df_long_sound (explode).\"\"\"\n",
    "    wavs = list(data_dir.rglob(\"*.wav\"))\n",
    "    rows = []\n",
    "    for p in wavs:\n",
    "        info = parse_filename(p.name)\n",
    "        if info:\n",
    "            info[\"filepath\"] = str(p)\n",
    "            rows.append(info)\n",
    "    df_wide = pd.DataFrame(rows)\n",
    "\n",
    "    if not df_wide.empty:\n",
    "        df_long_diag  = df_wide.explode(\"diagnoses\",   ignore_index=True).rename(columns={\"diagnoses\":\"diagnosis\"})\n",
    "        df_long_sound = df_wide.explode(\"sound_types\", ignore_index=True).rename(columns={\"sound_types\":\"sound_type\"})\n",
    "        df_long_diag[\"diagnosis\"]   = df_long_diag[\"diagnosis\"].fillna(\"\").astype(str).str.strip()\n",
    "        df_long_sound[\"sound_type\"] = df_long_sound[\"sound_type\"].fillna(\"\").astype(str).str.strip()\n",
    "        df_long_diag  = df_long_diag[df_long_diag[\"diagnosis\"]   != \"\"]\n",
    "        df_long_sound = df_long_sound[df_long_sound[\"sound_type\"] != \"\"]\n",
    "    else:\n",
    "        df_long_diag  = pd.DataFrame(columns=[\"filename\",\"diagnosis\"])\n",
    "        df_long_sound = pd.DataFrame(columns=[\"filename\",\"sound_type\"])\n",
    "\n",
    "    return df_wide, df_long_diag, df_long_sound\n",
    "\n",
    "# ----------------------------\n",
    "# 5) Frequencies + simple plots\n",
    "# ----------------------------\n",
    "def frequency_tables(df_wide: pd.DataFrame, df_long_diag: pd.DataFrame, df_long_sound: pd.DataFrame):\n",
    "    out = {}\n",
    "    if not df_wide.empty:\n",
    "        out[\"count_by_filter_code\"] = df_wide[\"filter_code\"].value_counts().rename_axis(\"filter_code\").to_frame(\"count\")\n",
    "        out[\"count_by_filter\"] = df_wide[\"filter\"].value_counts().rename_axis(\"filter\").to_frame(\"count\")\n",
    "        out[\"count_by_location_norm\"] = df_wide[\"location_norm\"].value_counts(dropna=False).rename_axis(\"location_norm\").to_frame(\"count\")\n",
    "        out[\"count_by_gender\"] = df_wide[\"gender\"].value_counts(dropna=False).rename_axis(\"gender\").to_frame(\"count\")\n",
    "        out[\"count_bad_location\"] = df_wide.loc[~df_wide[\"location_valid\"] | df_wide[\"location_norm\"].isna(), \"location_issue\"]\\\n",
    "                                          .value_counts().rename_axis(\"issue\").to_frame(\"count\")\n",
    "    else:\n",
    "        out[\"count_by_filter_code\"] = pd.DataFrame()\n",
    "        out[\"count_by_filter\"] = pd.DataFrame()\n",
    "        out[\"count_by_location_norm\"] = pd.DataFrame()\n",
    "        out[\"count_by_gender\"] = pd.DataFrame()\n",
    "        out[\"count_bad_location\"] = pd.DataFrame()\n",
    "\n",
    "    if not df_long_diag.empty:\n",
    "        diag_abs = df_long_diag[\"diagnosis\"].value_counts().rename(\"count\").to_frame()\n",
    "        diag_rel = (diag_abs[\"count\"]/diag_abs[\"count\"].sum()).rename(\"relative\").to_frame()\n",
    "        out[\"diagnosis_abs\"] = diag_abs\n",
    "        out[\"diagnosis_rel\"] = diag_rel\n",
    "    else:\n",
    "        out[\"diagnosis_abs\"] = pd.DataFrame(); out[\"diagnosis_rel\"] = pd.DataFrame()\n",
    "\n",
    "    if not df_long_sound.empty:\n",
    "        snd_abs = df_long_sound[\"sound_type\"].value_counts().rename(\"count\").to_frame()\n",
    "        snd_rel = (snd_abs[\"count\"]/snd_abs[\"count\"].sum()).rename(\"relative\").to_frame()\n",
    "        out[\"sound_abs\"] = snd_abs\n",
    "        out[\"sound_rel\"] = snd_rel\n",
    "    else:\n",
    "        out[\"sound_abs\"] = pd.DataFrame(); out[\"sound_rel\"] = pd.DataFrame()\n",
    "\n",
    "    return out\n",
    "\n",
    "def plot_bar(series: pd.Series, title: str, xlabel: str, ylabel: str, rotation: int = 0, fname: Optional[Path] = None):\n",
    "    plt.figure(figsize=(8,5))\n",
    "    series.plot(kind=\"bar\")\n",
    "    plt.title(title); plt.xlabel(xlabel); plt.ylabel(ylabel)\n",
    "    plt.xticks(rotation=rotation); plt.tight_layout()\n",
    "    if fname: plt.savefig(fname, dpi=150)\n",
    "    plt.show(); plt.close()\n",
    "\n",
    "def plot_age_vs_diagnosis_box(df_long_diag: pd.DataFrame, fname: Optional[Path] = None):\n",
    "    sub = df_long_diag.dropna(subset=[\"age\",\"diagnosis\"]).copy()\n",
    "    if sub.empty:\n",
    "        print(\"[WARN] No rows with age+diagnosis for boxplot.\"); return\n",
    "    diagnoses = list(sub[\"diagnosis\"].unique())\n",
    "    data = [sub.loc[sub[\"diagnosis\"]==d, \"age\"].values for d in diagnoses]\n",
    "    plt.figure(figsize=(10,5))\n",
    "    plt.boxplot(data, labels=diagnoses, showfliers=False)\n",
    "    plt.title(\"Age distribution by diagnosis (explode)\")\n",
    "    plt.xlabel(\"Diagnosis\"); plt.ylabel(\"Age (years)\")\n",
    "    plt.tight_layout()\n",
    "    if fname: plt.savefig(fname, dpi=150)\n",
    "    plt.show(); plt.close()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "#Download the dataset or check if it exists\n",
    "root_dir = download_and_extract(url=DATA_URL, work_dir=DATA_DIR)"
   ],
   "id": "2daae8db3a4e6bc0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "WAV_PATH = DATA_DIR / \"jwyy9np4gv-3\" / \"Audio Files\" # path to wav files\n",
    "df_wide, df_long_diag, df_long_sound = scan_dataset(WAV_PATH)\n",
    "\n",
    "# Save annotations\n",
    "df_wide.to_csv(OUTPUT_DIR / \"annotations_wide.csv\", index=False)\n",
    "df_long_diag.to_csv(OUTPUT_DIR / \"annotations_long.csv\", index=False)\n",
    "\n",
    "# Quick audit\n",
    "print(\"Parsed files:\", len(df_wide))\n",
    "print(\"Unique raw locations:\", df_wide[\"location_raw\"].nunique())\n",
    "print(\"Rows with invalid/incomplete location:\", (~df_wide[\"location_valid\"] | df_wide[\"location_norm\"].isna()).sum())\n",
    "print(\"Rows with 4+ tokens in location:\", df_wide[\"location_had_4_tokens\"].sum())\n",
    "\n",
    "# Inspect problematic locations\n",
    "bad = df_wide.loc[~df_wide[\"location_valid\"] | df_wide[\"location_norm\"].isna(),\n",
    "                  [\"filename\",\"location_raw\",\"location_issue\",\"location_tokens_count\"]]\n",
    "display(bad.head(20))\n",
    "\n"
   ],
   "id": "dac382552f7b4227",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Patient-Level Aggregation & Paper-Style Tables\n",
    "\n",
    "This block aggregates per **patient** and generates tables/plots that mirror the paper’s summaries.\n",
    "\n",
    "\n",
    "- **Patient-level aggregation**\n",
    "  - Group `df_wide` by `patient_id`.\n",
    "  - Merge and de-duplicate **sound types** and **diagnoses** per patient.\n",
    "  - Pick the first valid **location** per patient; title-case it.\n",
    "\n",
    "- **Table 1 — Chest zones (Location)**\n",
    "\n",
    "- **Table 2 — Sound types (patient profile)**\n",
    "\n",
    "- **Table 3 — Health condition (per diagnosis)**\n",
    "\n",
    "- **Interactive visualizations**\n",
    "  - **Boxplot (Plotly):** age distribution per diagnosis (shows points + box).\n",
    "  - **Bar chart (Plotly):** diagnosis frequencies (unique patients).\n",
    "  - **Co-occurrence heatmap (Seaborn):** pairwise counts of diagnoses across patients.\n",
    "\n",
    "\n"
   ],
   "id": "8f0ba28ea51b0ba8"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "SAVE_DIR = Path(\"./outputs_tables_like_paper\")\n",
    "SAVE_DIR.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "# ========= Helpers =========\n",
    "def _dedup(seq):\n",
    "    seen, out = set(), []\n",
    "    for x in seq:\n",
    "        if x not in seen:\n",
    "            seen.add(x); out.append(x)\n",
    "    return out\n",
    "\n",
    "def _sound_profile_label(sound_list):\n",
    "    \"\"\"\n",
    "    Map a patient's sound_types (unique) to the paper-like categories:\n",
    "      - 'Normal'\n",
    "      - 'Crepitations'  (we mapped Crep/Crackles → 'Crackles'; if usas 'Crackles' cámbialo aquí)\n",
    "      - 'Wheezes'\n",
    "      - 'Bronchial'\n",
    "      - 'Crackles'\n",
    "      - 'Wheezes & Crackles'\n",
    "      - 'Bronchial & Crackles'\n",
    "    \"\"\"\n",
    "    if not isinstance(sound_list, (list, tuple)) or len(sound_list) == 0:\n",
    "        return None\n",
    "    s = sorted(set(sound_list))\n",
    "    # Nombres estándar esperados\n",
    "    hasN  = \"Normal\"     in s\n",
    "    hasW  = \"Wheezes\"    in s\n",
    "    hasC  = \"Crackles\"   in s or \"Crepitations\" in s  \n",
    "    hasB  = \"Bronchial\"  in s\n",
    "\n",
    "    # Normal\n",
    "    if hasN and not (hasW or hasC or hasB):\n",
    "        return \"Normal\"\n",
    "\n",
    "    if hasW and hasC and not hasB:\n",
    "        return \"Wheezes & Crackles\"\n",
    "    if hasB and hasC and not hasW:\n",
    "        return \"Bronchial & Crackles\"\n",
    "\n",
    "    # Simples\n",
    "    if hasW and not (hasC or hasB):\n",
    "        return \"Wheezes\"\n",
    "    if hasC and not (hasW or hasB):\n",
    "        return \"Crepitations\"\n",
    "    if hasB and not (hasW or hasC):\n",
    "        return \"Bronchial\"\n",
    "\n",
    "    return \" & \".join(s)\n",
    "\n",
    "def _to_title(loc):\n",
    "    \"\"\"Ensure title-case for locations like 'Posterior right upper' -> 'Posterior Right Upper'.\"\"\"\n",
    "    return None if pd.isna(loc) else str(loc).title()\n",
    "\n",
    "# ========= Base: collapse by patient =========\n",
    "# Note: if you have multiple rows per patient with different locations/sounds/diagnoses,\n",
    "# we take the FIRST one for simplicity for 'location'; for 'sound_types' and 'diagnoses'\n",
    "# we merge and deduplicate.\n",
    "\n",
    "agg_rows = []\n",
    "for pid, g in df_wide.groupby(\"patient_id\", sort=False):\n",
    "    row = g.iloc[0].copy()\n",
    "    all_sounds = []\n",
    "    all_diags  = []\n",
    "    for _, r in g.iterrows():\n",
    "        if isinstance(r.get(\"sound_types\"), list):\n",
    "            all_sounds.extend(r[\"sound_types\"])\n",
    "        if isinstance(r.get(\"diagnoses\"), list):\n",
    "            all_diags.extend(r[\"diagnoses\"])\n",
    "    row[\"sound_types_patient\"] = _dedup([s for s in all_sounds if isinstance(s, str) and s.strip()])\n",
    "    row[\"diagnoses_patient\"]   = _dedup([d for d in all_diags  if isinstance(d, str) and d.strip()])\n",
    "    # location: if there are several different ones, prioritize the first valid one that appears\n",
    "    loc = g[\"location_norm\"].dropna()\n",
    "    row[\"location_patient\"] = loc.iloc[0] if not loc.empty else None\n",
    "    agg_rows.append(row)\n",
    "\n",
    "df_pat = pd.DataFrame(agg_rows)\n",
    "\n",
    "# Normalize location a Title-Case\n",
    "df_pat[\"location_patient\"] = df_pat[\"location_patient\"].map(_to_title)\n",
    "\n",
    "# ========= Tabla 1: Chest zones (Location) =========\n",
    "tbl_locations = (\n",
    "    df_pat[\"location_patient\"]\n",
    "    .dropna()\n",
    "    .value_counts()\n",
    "    .rename_axis(\"Location\")\n",
    "    .to_frame(\"No. of Subjects\")\n",
    "    .sort_index()\n",
    ")\n",
    "print(\"\\nTable 1 — Chest zones (unique patients)\")\n",
    "print(tbl_locations)\n",
    "tbl_locations.to_csv(SAVE_DIR / \"table1_locations.csv\")\n",
    "\n",
    "# ========= Table 2: Sound types  =========\n",
    "df_pat[\"sound_profile\"] = df_pat[\"sound_types_patient\"].map(_sound_profile_label)\n",
    "tbl_sound = (\n",
    "    df_pat[\"sound_profile\"]\n",
    "    .dropna()\n",
    "    .value_counts()\n",
    "    .rename_axis(\"Sound Type\")\n",
    "    .to_frame(\"No. of Subjects\")\n",
    ")\n",
    "# Sort\n",
    "order_sound = [\n",
    "    \"Normal\",\n",
    "    \"Crepitations\",      \n",
    "    \"Wheezes\",\n",
    "    \"Crackles\",          \n",
    "    \"Bronchial\",\n",
    "    \"Wheezes & Crackles\",\n",
    "    \"Bronchial & Crackles\",\n",
    "]\n",
    "tbl_sound = tbl_sound.reindex(order_sound + [i for i in tbl_sound.index if i not in order_sound]).dropna(how=\"all\")\n",
    "print(\"\\nTable 2 — Sound types (unique patients)\")\n",
    "print(tbl_sound)\n",
    "tbl_sound.to_csv(SAVE_DIR / \"table2_sound_types.csv\")\n",
    "\n",
    "# ========= Table 3: Health condition (count, age range, gender) =========\n",
    "# Explode diagnoses at the patient level, drop duplicates (a patient counts once per condition)\n",
    "\n",
    "tmp = df_pat.explode(\"diagnoses_patient\", ignore_index=True)\n",
    "tmp = tmp.rename(columns={\"diagnoses_patient\": \"diagnosis\"})\n",
    "tmp = tmp.dropna(subset=[\"diagnosis\"])\n",
    "tmp = tmp.drop_duplicates(subset=[\"patient_id\", \"diagnosis\"])\n",
    "\n",
    "def _gender_counts(s):\n",
    "    f = int((s == \"F\").sum())\n",
    "    m = int((s == \"M\").sum())\n",
    "    return f\"{f} female, {m} male\"\n",
    "\n",
    "agg = tmp.groupby(\"diagnosis\").agg(\n",
    "    **{\n",
    "        \"No. of Subjects\": (\"patient_id\", \"nunique\"),\n",
    "        \"Age Min\": (\"age\", \"min\"),\n",
    "        \"Age Max\": (\"age\", \"max\"),\n",
    "        \"Gender\": (\"gender\", _gender_counts),\n",
    "    }\n",
    ").reset_index()\n",
    "\n",
    "# Format age range in a single column\n",
    "agg[\"Age Range\"] = agg.apply(lambda r: f\"{int(r['Age Min'])}–{int(r['Age Max'])}\" if pd.notna(r[\"Age Min\"]) else \"NA\", axis=1)\n",
    "tbl_health = agg[[\"diagnosis\", \"No. of Subjects\", \"Age Range\", \"Gender\"]].rename(columns={\"diagnosis\": \"Health Condition\"})\n",
    "order_health = [\"Normal\",\"Asthma\",\"Pneumonia\",\"COPD\",\"BRON\",\"Heart Failure\",\"Lung Fibrosis\",\"Pleural Effusion\"]\n",
    "tbl_health = tbl_health.set_index(\"Health Condition\").reindex(order_health + [i for i in tbl_health[\"Health Condition\"].tolist() if i not in order_health]).dropna(how=\"all\")\n",
    "print(\"\\nTable 3 — Health condition (unique patients)\")\n",
    "print(tbl_health)\n",
    "tbl_health.to_csv(SAVE_DIR / \"table3_health_condition.csv\")\n",
    "\n",
    "\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "\n",
    "# --- Boxplot interactive Plotly ---\n",
    "fig = px.box(\n",
    "    df_long_diag,\n",
    "    x=\"diagnosis\",\n",
    "    y=\"age\",\n",
    "    points=\"all\",   # show each patient as a point in the boxplot plotly\n",
    "    title=\"Age distribution per diagnosis\"\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "# --- Interactive bars: diagnosis frequency ---\n",
    "freq_diag = df_long_diag[\"diagnosis\"].value_counts().reset_index()\n",
    "freq_diag.columns = [\"diagnosis\",\"count\"]\n",
    "\n",
    "fig = px.bar(\n",
    "    freq_diag,\n",
    "    x=\"diagnosis\",\n",
    "    y=\"count\",\n",
    "    title=\"Diagnosis frequency (unique patients)\",\n",
    "    text=\"count\"\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "# --- Heatmap Diagnosis co-ocurrences---\n",
    "from itertools import combinations\n",
    "import numpy as np\n",
    "\n",
    "# Build co-occurrence matrix\n",
    "diagnoses = df_long_diag[\"diagnosis\"].unique()\n",
    "cooc = pd.DataFrame(0, index=diagnoses, columns=diagnoses)\n",
    "\n",
    "for pid, g in df_long_diag.groupby(\"patient_id\"):\n",
    "    dlist = g[\"diagnosis\"].unique()\n",
    "    for a,b in combinations(dlist, 2):\n",
    "        cooc.loc[a,b] += 1\n",
    "        cooc.loc[b,a] += 1\n",
    "\n",
    "sns.heatmap(cooc, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "plt.title(\"Co-occurrence of diagnoses across patients\")\n",
    "plt.show()\n"
   ],
   "id": "8cebbb61ee4845ac",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "21cc857ad8d8b4fb"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Part 2- Feature Extraction",
   "id": "8abc91b204cafdd7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "** To build a dataframe with the audio data and other acquisition metadata **",
   "id": "36b9ce3c016929a4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Part 2(A) — Audio Loading & Labeling (Dataset → (data, label) frame)\n",
    "\n",
    "This block reads WAV files, assigns a single label per recording, and compiles a tidy DataFrame for downstream feature extraction and modeling.\n",
    "\n",
    "**What this cell does:**\n",
    "- **Label helper**\n",
    "  - `pick_label_from_row(r)`: chooses one label per recording  \n",
    "    - Priority: first item in `diagnoses`; else `\"Normal\"` if present in `sound_types`; else `\"Unknown\"`.\n",
    "\n",
    "- **WAV reader**\n",
    "  - `read_wav_info(path)`: loads audio with **soundfile**, converts stereo → mono (mean), returns  \n",
    "    `path, sample_rate, audio(float32), dtype, n_samples, duration_s`.\n",
    "\n",
    "- **Dataset → (data, label) DataFrame**\n",
    "  - `build_data_label_df(df_wide)`: iterates `df_wide` (must contain `filepath`, `filename`, `patient_id`, `filter_code`, `diagnoses`, `sound_types`), loads audio, assigns label, and returns a DataFrame with:\n",
    "    - `data` (np.array), `label`, `path`, `sample_rate`, `dtype`, `n_samples`, `duration_s`,\n",
    "    - plus traceability fields: `filename`, `patient_id`, `filter_code`.\n",
    "  - Warns and skips files that fail to load.\n",
    "\n",
    "- **Basic per-label statistics**\n",
    "  - `overall_stats_basic(df_audio)`: aggregates **count**, **mean duration**, **std duration** by `label` (rounded for readability).\n",
    "\n",
    "**Notes**\n",
    "- Audio is kept in original amplitude scale; conversion to mono ensures shape consistency.\n",
    "- Single-label strategy is intentional for Part 2; multi-label handling can be added later if needed.\n"
   ],
   "id": "4420e543a5066302"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "from pathlib import Path\n",
    "from typing import Dict, Any, List\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import soundfile as sf\n",
    "import librosa\n",
    "import re\n",
    "\n",
    "# ---------------------------------\n",
    "# Helpers: pick label from df_wide\n",
    "# ---------------------------------\n",
    "def pick_label_from_row(r: pd.Series):\n",
    "    \"\"\"\n",
    "    Choose a single label for the recording:\n",
    "    - If 'diagnoses' list has elements -> first one\n",
    "    - Else if sound_types contains 'Normal' -> 'Normal'\n",
    "    - Else -> 'Unknown'\n",
    "    \"\"\"\n",
    "    diag = r.get(\"diagnoses\", None)\n",
    "    if isinstance(diag, list) and len(diag) > 0:\n",
    "        # Prioritize first diagnosis (keeps it simple and stable for part 2)\n",
    "        return str(diag[0])\n",
    "    # fallback: Normal if appears as sound type\n",
    "    st = r.get(\"sound_types\", None)\n",
    "    if isinstance(st, list) and any(s == \"Normal\" for s in st):\n",
    "        return \"Normal\"\n",
    "    return \"Unknown\"\n",
    "\n",
    "# ---------------------------------\n",
    "# WAV reader\n",
    "# ---------------------------------\n",
    "def read_wav_info(path: str):\n",
    "    \"\"\"\n",
    "    Read a WAV with soundfile, convert to mono if needed.\n",
    "    Returns a dict with path, sample_rate, audio (float32), dtype, n_samples, duration_s.\n",
    "    \"\"\"\n",
    "    audio, sr = sf.read(path, always_2d=False)\n",
    "    if audio.ndim == 2:  # stereo -> mono\n",
    "        audio = audio.mean(axis=1)\n",
    "    info = {\n",
    "        \"path\": path,\n",
    "        \"sample_rate\": int(sr),\n",
    "        \"audio\": audio.astype(np.float32, copy=False),\n",
    "        \"dtype\": str(audio.dtype),\n",
    "        \"n_samples\": int(audio.shape[0]),\n",
    "        \"duration_s\": float(audio.shape[0] / sr),\n",
    "    }\n",
    "    return info\n",
    "\n",
    "# ---------------------------------\n",
    "# Build (data, label) dataframe\n",
    "# ---------------------------------\n",
    "def build_data_label_df(df_wide: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Iterate df_wide rows, read audio, attach a single label from 'diagnoses'.\n",
    "    Output: DataFrame with columns: data (np.array), label, path, sample_rate, dtype, n_samples, duration_s.\n",
    "    \"\"\"\n",
    "    rows: List[Dict[str, Any]] = []\n",
    "    for _, r in df_wide.iterrows():\n",
    "        path = r[\"filepath\"]\n",
    "        try:\n",
    "            wav = read_wav_info(path)\n",
    "            label = pick_label_from_row(r)\n",
    "            rows.append({\n",
    "                \"data\": wav[\"audio\"],\n",
    "                \"label\": label,\n",
    "                \"path\": wav[\"path\"],\n",
    "                \"sample_rate\": wav[\"sample_rate\"],\n",
    "                \"dtype\": wav[\"dtype\"],\n",
    "                \"n_samples\": wav[\"n_samples\"],\n",
    "                \"duration_s\": wav[\"duration_s\"],\n",
    "                # opcional: metadatos útiles para trazabilidad\n",
    "                \"filename\": r[\"filename\"],\n",
    "                \"patient_id\": r[\"patient_id\"],\n",
    "                \"filter_code\": r[\"filter_code\"],\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(f\"[WARN] Failed on {Path(path).name}: {e}\")\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "\n",
    "def overall_stats_basic(df_audio: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Compute overall statistics per label (diagnosis).\n",
    "    Minimal version for Part 2(A): count, duration mean and std.\n",
    "    \"\"\"\n",
    "    stats = (\n",
    "        df_audio.groupby(\"label\")\n",
    "        .agg(\n",
    "            count=(\"duration_s\", \"count\"),\n",
    "            dur_mean=(\"duration_s\", \"mean\"),\n",
    "            dur_std=(\"duration_s\", \"std\"),\n",
    "        )\n",
    "        .sort_values(\"count\", ascending=False)\n",
    "        .reset_index()\n",
    "    )\n",
    "    # rounding for readability\n",
    "    stats[\"dur_mean\"] = stats[\"dur_mean\"].round(3)\n",
    "    stats[\"dur_std\"] = stats[\"dur_std\"].round(3)\n",
    "    return stats\n"
   ],
   "id": "a59dc6044884825c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "df_audio = build_data_label_df(df_wide)\n",
    "\n",
    "stats_table = overall_stats_basic(df_audio)\n",
    "\n",
    "print(\"Overall Statistical Analysis (basic):\")\n",
    "print(stats_table)\n",
    "\n",
    "# --- Checking sample rate ---\n",
    "unique_sr = df_audio[\"sample_rate\"].unique()\n",
    "expected_sr=4000\n",
    "if len(unique_sr) > 1:\n",
    "    print(f\"[WARN] Found multiple sampling rates: {unique_sr}\")\n",
    "elif expected_sr is not None and unique_sr[0] != expected_sr:\n",
    "    print(f\"[WARN] Sampling rate {unique_sr[0]} does not match expected {expected_sr}\")\n",
    "else:\n",
    "    print(f\"[INFO] All files have consistent sampling rate: {unique_sr[0]}\")\n"
   ],
   "id": "f506933b5ef81383",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "**part 2B** Visualize Spectograms MEL, WAVELETS MORLET AND MEX HAT ",
   "id": "58e3698a998489ff"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "from ipywidgets import interact, Dropdown, IntSlider\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import librosa, librosa.display\n",
    "\n",
    "SAMPLE_RATE = df_audio[\"sample_rate\"].iloc[0]\n",
    "FIXED_LEN   = 16000 # number of samples\n",
    "N_MELS      = 64   # number of bands in mel scale\n",
    "N_FFT       = 512  # window size in FFT must be a power of two TODO If not power of two use the nearest 2^n\n",
    "HOP         = 256  # number of samples to jump in each window \n",
    "\n",
    "def fix_length_16000(y, L=FIXED_LEN): # padding\n",
    "    if len(y) < L:\n",
    "        y = np.pad(y, (0, L-len(y)), mode=\"constant\")\n",
    "    else:\n",
    "        y = y[:L]\n",
    "    return y.astype(np.float32, copy=False)\n",
    "\n",
    "def compute_mel_spectrogram(y, sr=SAMPLE_RATE, n_mels=N_MELS, n_fft=N_FFT, hop=HOP):\n",
    "    S = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=n_mels, n_fft=n_fft, hop_length=hop, fmax=sr//2)\n",
    "    return librosa.power_to_db(S, ref=np.max)\n",
    "\n",
    "def _build_index(df_audio):\n",
    "    d = {}\n",
    "    for i, r in df_audio.iterrows():\n",
    "        lab = str(r[\"label\"])\n",
    "        d.setdefault(lab, []).append(i)\n",
    "    return {k: d[k] for k in sorted(d)}\n",
    "\n"
   ],
   "id": "2891f0c3f31c99be",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "def view_sample(df_audio, label, index=0):\n",
    "    # filter per label and draw index\n",
    "    sub = df_audio[df_audio[\"label\"] == label]\n",
    "    if sub.empty:\n",
    "        print(f\"No samples for label={label}\"); return\n",
    "    index = max(0, min(index, len(sub)-1))\n",
    "    row = sub.iloc[index]\n",
    "    y = fix_length_16000(row[\"data\"])\n",
    "    sr = int(row.get(\"sample_rate\", SAMPLE_RATE))\n",
    "\n",
    "    plt.figure(figsize=(11,3))\n",
    "    # Waveform\n",
    "    plt.subplot(1,2,1)\n",
    "    t = np.arange(len(y))/sr\n",
    "    plt.plot(t, y, linewidth=1)\n",
    "    plt.title(f\"Waveform — {label}\")\n",
    "    plt.xlabel(\"Time (s)\"); plt.ylabel(\"Amplitude\"); plt.grid(True, alpha=0.2)\n",
    "    # Spectrogram\n",
    "    plt.subplot(1,2,2)\n",
    "    S_db = compute_mel_spectrogram(y, sr)\n",
    "    librosa.display.specshow(S_db, sr=sr, hop_length=HOP, x_axis=\"time\", y_axis=\"mel\", fmax=sr/2)\n",
    "    plt.title(f\"Mel spectrogram — {label}\")\n",
    "    plt.colorbar(format=\"%+2.0f dB\")\n",
    "    plt.tight_layout(); plt.show()\n",
    "\n",
    "# ejemplo:\n",
    "view_sample(df_audio, \"Normal\", index=0)\n",
    "view_sample(df_audio, \"COPD\", index=0)\n",
    "view_sample(df_audio, \"BRON\", index=0)\n",
    "view_sample(df_audio, \"Asthma\", index=0)\n",
    "view_sample(df_audio, \"Pneumonia\", index=0)\n",
    "view_sample(df_audio, \"Heart Failure\", index=0)\n",
    "view_sample(df_audio, \"Plueral Effusion\", index=0)"
   ],
   "id": "4589d87a3f179db8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Part 2(B) — Interactive Waveform & Spectrogram Browser (16k samples)\n",
    "\n",
    "This block builds an **interactive viewer** to explore waveforms and time–frequency representations (Mel and CWT) per label, aligned to the requirement of using **samples of length = 16,000** (with trimming and padding when needed).\n",
    "\n",
    "- **Preprocessing**\n",
    "  - Trim leading silence/noise: skips the first `LEAD_TRIM_S` seconds per clip.\n",
    "  - Z-score **amplitude normalization** for comparability across recordings.\n",
    "  - Fix length to **16,000 samples** (pad/truncate).\n",
    "\n",
    "- **Representations**\n",
    "  - **Mel spectrogram**: `librosa.feature.melspectrogram` → dB scale (baseline view).\n",
    "  - **Continuous Wavelet Transform (CWT)**:\n",
    "    - **Morlet** and **Ricker** wavelets with frequency axis shown in **log-Hz**.\n",
    "\n",
    "- **Interactive controls (ipywidgets)**\n",
    "  - **Label** dropdown: choose a diagnosis label.\n",
    "  - **Index** slider: iterate through recordings within the selected label.\n",
    "  - **View** dropdown: switch between *Mel*, *Morlet (CWT)*, and *Ricker (CWT)*.\n",
    "\n",
    "- **Plots**\n",
    "  - **Waveform** (top-left): z-scored amplitude vs. time.\n",
    "  - **Time–frequency map** (top-right): Mel / Morlet-CWT / Ricker-CWT with colorbar.\n",
    "  - Layout uses `matplotlib` and `librosa.display` (for Mel).\n",
    "\n",
    "**Assumptions**\n",
    "- `df_audio` exists and contains at least: `data` (np.array), `label`, `filename`, `sample_rate`.\n",
    "- Uses a single, consistent `SAMPLE_RATE` from `df_audio`.\n",
    "- Dependencies: `matplotlib`, `librosa`, `scipy`, `ipywidgets` (ensure Jupyter widgets are enabled).\n",
    "\n",
    "**Notes**\n",
    "- Adjust `LEAD_TRIM_S`, `N_MELS`, `N_FFT`, `HOP`, and CWT scale ranges to match your dataset characteristics.\n",
    "- The log-frequency edges for CWT are derived from scale center frequencies to avoid band overlap artifacts."
   ],
   "id": "ab46c2bf1ae95507"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa, librosa.display\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import Dropdown, IntSlider\n",
    "from IPython.display import display, clear_output\n",
    "from scipy.signal import cwt, morlet2, ricker  # CWT\n",
    "\n",
    "# ------------ utils ------------\n",
    "SAMPLE_RATE = int(df_audio[\"sample_rate\"].iloc[0])  # p.ej. 4000\n",
    "FIXED_LEN   = 16000\n",
    "N_MELS      = 128\n",
    "N_FFT       = 256\n",
    "HOP         = 128\n",
    "LEAD_TRIM_S = 0.6  # seconds to trim at the beginning of each audio\n"
   ],
   "id": "e68464d19fca9d62",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "\n",
    "def trim_leading_seconds(y: np.ndarray, sr: int, secs: float = LEAD_TRIM_S):\n",
    "    \"\"\"Trim the first 'secs' seconds; if shorter, return an empty array.\"\"\"\n",
    "\n",
    "    off = int(round(secs * sr))\n",
    "    if off <= 0:\n",
    "        return y\n",
    "    return y[off:] if off < len(y) else np.array([], dtype=y.dtype)\n",
    "\n",
    "def zscore_normalize(y):  # normalize amplitude to compare signals\n",
    "\n",
    "    return (y - np.mean(y)) / (np.std(y) + 1e-8)\n",
    "\n",
    "def fix_length_16000(y, L=FIXED_LEN):\n",
    "    return (np.pad(y, (0, L-len(y)), mode=\"constant\") if len(y) < L else y[:L]).astype(np.float32, copy=False)\n",
    "\n",
    "# --- Mel (baseline) ---\n",
    "def compute_mel_spectrogram(y, sr=SAMPLE_RATE, n_mels=N_MELS, n_fft=N_FFT, hop=HOP):\n",
    "    S = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=n_mels, n_fft=n_fft, hop_length=hop, fmax=sr//2)\n",
    "    return librosa.power_to_db(S, ref=np.max)\n",
    "\n",
    "# --- Wavelets (Morlet / Ricker) ---\n",
    "def compute_cwt_morlet(y, sr=SAMPLE_RATE, w=6.0, n_scales=128, min_period_ms=12.5, max_period_ms=500.0):\n",
    "    # period (ms) -> scales (a, samplings)\n",
    "    min_scale = max(1, int(np.round((min_period_ms/1000.0) * sr * w / (2*np.pi))))\n",
    "    max_scale = int(np.round((max_period_ms/1000.0) * sr * w / (2*np.pi)))\n",
    "    widths = np.unique(np.linspace(min_scale, max_scale, n_scales).astype(int))\n",
    "    coef = cwt(y, morlet2, widths, w=w)        # (n_scales, T)\n",
    "    power = np.abs(coef)**2\n",
    "    f_hz = (w * sr) / (2*np.pi*widths)         # Hz aprox\n",
    "    return power, f_hz\n",
    "\n",
    "def compute_cwt_ricker(y, sr=SAMPLE_RATE, n_scales=128, min_period_ms=4.0, max_period_ms=200.0):\n",
    "    min_scale = max(1, int(np.round((min_period_ms/1000.0) * sr / 2.0)))\n",
    "    max_scale = int(np.round((max_period_ms/1000.0) * sr / 2.0))\n",
    "    widths = np.unique(np.linspace(min_scale, max_scale, n_scales).astype(int))\n",
    "    coef = cwt(y, ricker, widths)              # (n_scales, T)\n",
    "    power = np.abs(coef)**2\n",
    "    f_hz = sr / (2*np.pi*widths)               # Hz aprox\n",
    "    return power, f_hz\n",
    "\n",
    "# ---- helper: bordes log para pcolormesh en frecuencia ----\n",
    "def _freq_edges_from_centers_log(f_centers_hz: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Generate log-spaced edges from centers (Hz). f_centers_hz > 0.\"\"\"\n",
    "    f = np.sort(np.asarray(f_centers_hz))          # ascendente\n",
    "    # geometric midpoints between centers\n",
    "\n",
    "    mids = np.sqrt(f[:-1] * f[1:])\n",
    "    edges = np.empty(f.size + 1, dtype=float)\n",
    "    edges[1:-1] = mids\n",
    "    ratio0 = f[1] / f[0]\n",
    "    ration = f[-1] / f[-2]\n",
    "    edges[0] = f[0] / np.sqrt(ratio0)              # extrapolate down\n",
    "    edges[-1] = f[-1] * np.sqrt(ration)            # extrapolate up\n",
    "    return edges\n",
    "\n",
    "def _build_index(df_audio):\n",
    "    d = {}\n",
    "    for i, r in df_audio.iterrows():\n",
    "        lab = str(r[\"label\"])\n",
    "        d.setdefault(lab, []).append(i)\n",
    "    return {k: d[k] for k in sorted(d)}\n",
    "\n",
    "# ------------ Interactive Plot (log-Hz en wavelets) ------------\n",
    "def show_browser(df_audio):\n",
    "    idx_map = _build_index(df_audio)\n",
    "    labels = list(idx_map.keys())\n",
    "\n",
    "    dd_label = Dropdown(options=labels, value=labels[0], description=\"Label:\")\n",
    "    slider   = IntSlider(min=0, max=max(0, len(idx_map[dd_label.value])-1), step=1, value=0, description=\"Index:\")\n",
    "    dd_repr  = Dropdown(options=[(\"Mel\", \"mel\"), (\"Morlet (CWT)\", \"morlet\"), (\"Ricker (CWT)\", \"ricker\")],\n",
    "                        value=\"mel\", description=\"View:\")\n",
    "    out = widgets.Output()\n",
    "\n",
    "    def _redraw(*_):\n",
    "        label = dd_label.value\n",
    "        i_list = idx_map[label]\n",
    "        idx = min(max(slider.value, 0), len(i_list)-1)\n",
    "        slider.value = idx\n",
    "\n",
    "        row = df_audio.loc[i_list[idx]]\n",
    "        fname = str(row.get(\"filename\", \"unknown\"))\n",
    "        sr = int(row.get(\"sample_rate\", SAMPLE_RATE))\n",
    "\n",
    "        y  = fix_length_16000(zscore_normalize(trim_leading_seconds(row[\"data\"],sr, secs=LEAD_TRIM_S))) # skip first 0.6seg then z-normalize and fix length \n",
    "        t  = np.arange(len(y)) / sr\n",
    "\n",
    "        with out:\n",
    "            clear_output(wait=True)\n",
    "            plt.figure(figsize=(12, 6))\n",
    "\n",
    "            # Waveform\n",
    "            ax1 = plt.subplot(2, 2, 1)\n",
    "            ax1.plot(t, y, linewidth=1)\n",
    "            ax1.set_title(f\"{fname} — Waveform (z-score) — {label}\")\n",
    "            ax1.set_xlabel(\"Time (s)\"); ax1.set_ylabel(\"Amplitude (z)\")\n",
    "            ax1.grid(True, alpha=0.2)\n",
    "\n",
    "            # Espectro / Wavelet (columna derecha arriba)\n",
    "            ax2 = plt.subplot(2, 2, 2)\n",
    "\n",
    "            if dd_repr.value == \"mel\":\n",
    "                ax2.cla()\n",
    "\n",
    "                S_db = compute_mel_spectrogram(y, sr)\n",
    "                im = librosa.display.specshow(\n",
    "                    S_db, sr=sr, hop_length=HOP,\n",
    "                    x_axis=\"time\", y_axis=\"mel\", fmax=sr/2, ax=ax2\n",
    "                )\n",
    "                ax2.set_title(f\"Mel spectrogram — {label}\")\n",
    "                plt.colorbar(im, ax=ax2, format=\"%+2.0f dB\")\n",
    "\n",
    "            elif dd_repr.value == \"morlet\":\n",
    "                power, f_hz = compute_cwt_morlet(y, sr=sr, w=6.0, n_scales=64, min_period_ms=12.5, max_period_ms=500.0)\n",
    "                # ordenar por frecuencia ascendente para malla log\n",
    "                order = np.argsort(f_hz)\n",
    "                f_ord = f_hz[order]\n",
    "                P_ord = power[order, :]\n",
    "                # bordes en tiempo y frecuencia\n",
    "                t_edges = np.linspace(0, len(y)/sr, P_ord.shape[1] + 1)\n",
    "                f_edges = _freq_edges_from_centers_log(f_ord)\n",
    "                pm = ax2.pcolormesh(t_edges, f_edges, P_ord, shading=\"auto\")\n",
    "                ax2.set_yscale(\"log\"); ax2.set_ylim(f_edges[0], f_edges[-1])\n",
    "                ax2.set_title(f\"CWT Morlet power — {label}\")\n",
    "                ax2.set_xlabel(\"Time (s)\"); ax2.set_ylabel(\"Frequency (Hz)\")\n",
    "                plt.colorbar(pm, ax=ax2)\n",
    "\n",
    "            else:  # ricker\n",
    "                power, f_hz = compute_cwt_ricker(y, sr=sr, n_scales=64, min_period_ms=4.0, max_period_ms=200.0)\n",
    "                order = np.argsort(f_hz)\n",
    "                f_ord = f_hz[order]\n",
    "                P_ord = power[order, :]\n",
    "                t_edges = np.linspace(0, len(y)/sr, P_ord.shape[1] + 1)\n",
    "                f_edges = _freq_edges_from_centers_log(f_ord)\n",
    "                pm = ax2.pcolormesh(t_edges, f_edges, P_ord, shading=\"auto\")\n",
    "                ax2.set_yscale(\"log\"); ax2.set_ylim(f_edges[0], f_edges[-1])\n",
    "                ax2.set_title(f\"CWT Ricker power — {label}\")\n",
    "                ax2.set_xlabel(\"Time (s)\"); ax2.set_ylabel(\"Frequency (Hz)\")\n",
    "                plt.colorbar(pm, ax=ax2)\n",
    "\n",
    "            # segunda fila: misma vista que arriba para comparar otro índice rápido (opcional)\n",
    "            ax3 = plt.subplot(2, 2, 3)\n",
    "            ax3.axis(\"off\")\n",
    "            ax4 = plt.subplot(2, 2, 4)\n",
    "            ax4.axis(\"off\")\n",
    "\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "    def _on_label_change(change):\n",
    "        if change[\"name\"] == \"value\":\n",
    "            n = len(idx_map[change[\"new\"]])\n",
    "            slider.max = max(0, n-1)\n",
    "            slider.value = 0\n",
    "            _redraw()\n",
    "\n",
    "    dd_label.observe(_on_label_change, names=\"value\")\n",
    "    slider.observe(_redraw, names=\"value\")\n",
    "    dd_repr.observe(_redraw, names=\"value\")\n",
    "\n",
    "    _redraw()\n",
    "    display(widgets.HBox([dd_label, slider, dd_repr]), out)\n"
   ],
   "id": "47ce843ba5791dca",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "show_browser(df_audio)\n",
    "show_browser(df_audio)\n"
   ],
   "id": "8eb010e79395fdb5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "**PART C** Extract Audio Features ",
   "id": "5258631fa8a061cf"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.signal import hilbert\n",
    "from scipy.stats import skew, kurtosis\n",
    "\n",
    "\n",
    "def pad_or_trim(y: np.ndarray, L: int = FIXED_LEN):\n",
    "    if len(y) < L:\n",
    "        y = np.pad(y, (0, L-len(y)), mode=\"constant\")\n",
    "    else:\n",
    "        y = y[:L]\n",
    "    return y.astype(np.float32, copy=False)\n",
    "\n",
    "# ============================\n",
    "# Helpers de features\n",
    "# ============================\n",
    "def amplitude_envelope(y: np.ndarray):\n",
    "    \"\"\"\n",
    "    Envelope por transformada de Hilbert (magnitud de la señal analítica).\n",
    "    \"\"\"\n",
    "    if y.size == 0:\n",
    "        return np.zeros(1, dtype=np.float32)\n",
    "    return np.abs(hilbert(y))\n",
    "\n",
    "def crest_factor(y: np.ndarray):\n",
    "    \"\"\"\n",
    "    Crest factor = peak / RMS.\n",
    "    \"\"\"\n",
    "    if y.size == 0:\n",
    "        return 0.0\n",
    "    peak = np.max(np.abs(y))\n",
    "    rms  = np.sqrt(np.mean(y**2) + 1e-12)\n",
    "    return float(peak / (rms + 1e-12))\n",
    "\n",
    "def peak_to_average(y: np.ndarray):\n",
    "    \"\"\"\n",
    "    Peak-to-Average (peak / mean(|y|)).\n",
    "    \"\"\"\n",
    "    if y.size == 0:\n",
    "        return 0.0\n",
    "    peak = np.max(np.abs(y))\n",
    "    avg  = np.mean(np.abs(y)) + 1e-12\n",
    "    return float(peak / avg)\n",
    "\n",
    "def rms_energy(y: np.ndarray):\n",
    "    \"\"\"\n",
    "    Devuelve (RMS, Energy=sum(y^2)).\n",
    "    \"\"\"\n",
    "    if y.size == 0:\n",
    "        return 0.0, 0.0\n",
    "    e = float(np.sum(y**2))\n",
    "    r = float(np.sqrt(np.mean(y**2) + 1e-12))\n",
    "    return r, e\n",
    "\n",
    "def zero_crossing_rate(y: np.ndarray):\n",
    "    \"\"\"\n",
    "    ZCR global (proporción de cambios de signo).\n",
    "    \"\"\"\n",
    "    if y.size < 2:\n",
    "        return 0.0\n",
    "    s = np.sign(y)\n",
    "    s[s==0] = 1\n",
    "    zc = np.sum(s[:-1] * s[1:] < 0)\n",
    "    return float(zc / (y.size - 1))\n",
    "\n",
    "def robust_stats(y: np.ndarray):\n",
    "    \"\"\"\n",
    "    Estadísticas globales: mean, std, median, mad, skew, kurt, p10, p90.\n",
    "    \"\"\"\n",
    "    if y.size == 0:\n",
    "        return dict(mean=0, std=0, median=0, mad=0, skew=0, kurt=0, p10=0, p90=0)\n",
    "    mean_ = float(np.mean(y))\n",
    "    std_  = float(np.std(y))\n",
    "    med   = float(np.median(y))\n",
    "    mad   = float(np.median(np.abs(y - med)))\n",
    "    sk    = float(skew(y, bias=False))\n",
    "    ku    = float(kurtosis(y, fisher=True, bias=False))\n",
    "    p10   = float(np.percentile(y, 10))\n",
    "    p90   = float(np.percentile(y, 90))\n",
    "    return dict(mean=mean_, std=std_, median=med, mad=mad, skew=sk, kurt=ku, p10=p10, p90=p90)\n",
    "\n",
    "def autocorr_features(y: np.ndarray, sr: int):\n",
    "    \"\"\"\n",
    "    Normalized autocorrelation and extraction of:\n",
    "    - max_acf (excluding lag 0),\n",
    "    - lag_peak (first strong peak above a threshold),\n",
    "    - f0_est (sr / lag_peak) if applicable.\n",
    "    \"\"\"\n",
    "\n",
    "    if y.size < 4:\n",
    "        return dict(max_acf=0.0, lag_peak=0, f0_est=0.0)\n",
    "\n",
    "    # 'full' autocorrelation, normalized by var\n",
    "    y0 = y - np.mean(y)\n",
    "    denom = (np.sum(y0**2) + 1e-12)\n",
    "    acf_full = np.correlate(y0, y0, mode='full') / denom\n",
    "    acf = acf_full[acf_full.size//2:]  # lags >= 0\n",
    "\n",
    "    # Ignore lag=0, look for peaks from a minimum lag (avoid micro-lags)\n",
    "    min_lag = max(1, int(0.005 * sr))   # 5 ms\n",
    "    max_lag = min(len(acf)-1, int(1.0 * sr))  # hasta 1 s\n",
    "\n",
    "    if max_lag <= min_lag:\n",
    "        return dict(max_acf=0.0, lag_peak=0, f0_est=0.0)\n",
    "\n",
    "    seg = acf[min_lag:max_lag]\n",
    "    max_acf = float(np.max(seg))\n",
    "    lag_peak_rel = int(np.argmax(seg))  # índice dentro de 'seg'\n",
    "    lag_peak = lag_peak_rel + min_lag\n",
    "\n",
    "    f0 = float(sr / lag_peak) if lag_peak > 0 else 0.0\n",
    "    return dict(max_acf=max_acf, lag_peak=int(lag_peak), f0_est=f0)\n",
    "\n",
    "def hjorth_parameters(y: np.ndarray, sr: int) -> dict:\n",
    "    \"\"\"\n",
    "    Hjorth: activity (var), mobility, complexity.\n",
    "    mobility = sqrt(var(dy)/var(y))\n",
    "    complexity = mobility(d2y)/mobility(dy)\n",
    "    \"\"\"\n",
    "    if y.size < 3:\n",
    "        return dict(hj_activity=0.0, hj_mobility=0.0, hj_complexity=0.0)\n",
    "\n",
    "    y0 = y - np.mean(y)\n",
    "    var0 = np.var(y0) + 1e-12\n",
    "\n",
    "    dy = np.diff(y0)\n",
    "    var1 = np.var(dy) + 1e-12\n",
    "\n",
    "    d2y = np.diff(dy)\n",
    "    var2 = np.var(d2y) + 1e-12\n",
    "\n",
    "    mobility = np.sqrt(var1 / var0)\n",
    "    mobility2 = np.sqrt(var2 / var1)\n",
    "    complexity = mobility2 / (mobility + 1e-12)\n",
    "\n",
    "    return dict(hj_activity=float(var0), hj_mobility=float(mobility), hj_complexity=float(complexity))\n",
    "\n",
    "def envelope_vs_finestructure(y: np.ndarray):\n",
    "    \"\"\"\n",
    "    Compare energy of the envelope (Hilbert) vs signal:\n",
    "    - env_rms, env_mean, env_p90\n",
    "    - env_to_signal_rms_ratio = RMS(env) / RMS(y)\n",
    "    - modulation_index = std(env) / mean(env)\n",
    "    \"\"\"\n",
    "\n",
    "    if y.size == 0:\n",
    "        return dict(env_rms=0.0, env_mean=0.0, env_p90=0.0,\n",
    "                    env_to_signal_rms_ratio=0.0, env_mod_index=0.0)\n",
    "\n",
    "    env = amplitude_envelope(y)\n",
    "    env_rms = float(np.sqrt(np.mean(env**2) + 1e-12))\n",
    "    env_mean = float(np.mean(env))\n",
    "    env_p90 = float(np.percentile(env, 90))\n",
    "    sig_rms = float(np.sqrt(np.mean(y**2) + 1e-12))\n",
    "    ratio = float(env_rms / (sig_rms + 1e-12))\n",
    "    mod_index = float(np.std(env) / (np.mean(env) + 1e-12))\n",
    "\n",
    "    return dict(env_rms=env_rms, env_mean=env_mean, env_p90=env_p90,\n",
    "                env_to_signal_rms_ratio=ratio, env_mod_index=mod_index)\n",
    "\n",
    "# ============================\n",
    "# Main Extractor\n",
    "# ============================\n",
    "def extract_classic_features(y_raw: np.ndarray, sr: int,\n",
    "                             lead_trim_s: float = LEAD_TRIM_S,\n",
    "                             fixed_len: int = FIXED_LEN):\n",
    "    \"\"\"\n",
    "    Pipeline:\n",
    "      1) trim lead (secs)\n",
    "      2) z-score\n",
    "      3) fix length\n",
    "      4) compute features\n",
    "    Returns: dict with all features.\n",
    "    \"\"\"\n",
    "\n",
    "    # 1) lead trim\n",
    "    y = trim_leading_seconds(y_raw, sr, secs=lead_trim_s)\n",
    "    # 2) z-score\n",
    "    y = zscore_normalize(y) if y.size else y\n",
    "    # 3) longitud fija\n",
    "    y = pad_or_trim(y, L=fixed_len)\n",
    "\n",
    "    feats = {}\n",
    "\n",
    "    # Envelope \n",
    "    feats.update(envelope_vs_finestructure(y))\n",
    "\n",
    "    # Potencia y picos\n",
    "    rms_, energy_ = rms_energy(y)\n",
    "    feats[\"rms\"] = float(rms_)\n",
    "    feats[\"energy\"] = float(energy_)\n",
    "    feats[\"crest_factor\"] = crest_factor(y)\n",
    "    feats[\"peak_to_avg\"] = peak_to_average(y)\n",
    "\n",
    "    # ZCR\n",
    "    feats[\"zcr\"] = zero_crossing_rate(y)\n",
    "\n",
    "    # Estadísticas de amplitud\n",
    "    feats.update({f\"amp_{k}\": v for k, v in robust_stats(y).items()})\n",
    "\n",
    "    # Autocorrelación y f0\n",
    "    feats.update(autocorr_features(y, sr))\n",
    "\n",
    "    # Hjorth\n",
    "    feats.update(hjorth_parameters(y, sr))\n",
    "\n",
    "    return feats\n"
   ],
   "id": "98f6d23725fa707b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "##Extract Spectral Features##\n",
    "# Part 2(C–D) — Spectral Feature Extraction (MFCC + STFT Stats)\n",
    "\n",
    "This block extracts compact **spectral descriptors** per recording to feed classical ML models.\n",
    "\n",
    "**What this cell does:**\n",
    "- **MFCCs (D):**  \n",
    "  - Compute **N_MFCC = 20** coefficients via `librosa.feature.mfcc` (`n_fft=512`, `hop=256`).  \n",
    "  - Aggregate per-coefficient **mean** and **std** → features: `mfcc{1..20}_mean`, `mfcc{1..20}_std`.\n",
    "\n",
    "- **STFT Log-Power Stats (C):**  \n",
    "  - Compute magnitude-squared STFT → log-power (`librosa.power_to_db`).  \n",
    "  - Global statistics over all time–freq bins:  \n",
    "    - `spec_mean`, `spec_std`, **skewness**, **kurtosis**.\n",
    "\n",
    "- **Outputs:**\n",
    "  - Builds `df_feats_spectral` with one row per recording containing:  \n",
    "    - Spectral features (MFCC means/stds + STFT stats).  \n",
    "    - Metadata: `idx`, `filename`, `mode` (filter code), `label`, `patient_id`, `sample_rate`.\n",
    "\n",
    "**Assumptions & notes**\n",
    "- Expects `df_audio` with columns: `data` (np.array), `sample_rate`, `filename`, `filter_code`, `label`, `patient_id`.\n",
    "- Amplitude scaling / fixed-length normalization should be applied **upstream** (e.g., z-score, 16k samples) for comparability.\n",
    "- Statistical moments use `scipy.stats` (`skew`, `kurtosis`) over flattened spectrogram bins.\n"
   ],
   "id": "31f651073eaef146"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import librosa\n",
    "from scipy.stats import skew, kurtosis\n",
    "\n",
    "EPS = 1e-10\n",
    "\n",
    "def clean_vec(x):\n",
    "    x = np.asarray(x).ravel()\n",
    "    x = x[np.isfinite(x)]\n",
    "    if x.size == 0:\n",
    "        return np.array([0.0], dtype=float)\n",
    "    return x\n",
    "\n",
    "def power_to_db_safe(S):\n",
    "    \"\"\"avoid -inf/NaN\"\"\"\n",
    "    S = np.asarray(S)\n",
    "    S = np.maximum(S, EPS)          # clamp\n",
    "    ref = S.max()\n",
    "    if not np.isfinite(ref) or ref < EPS:\n",
    "        ref = 1.0                   \n",
    "    S_db = librosa.power_to_db(S, ref=ref, amin=EPS, top_db=None)\n",
    "    # Asegurarfinitos:\n",
    "    S_db = np.nan_to_num(S_db, neginf=-120.0, posinf=0.0)\n",
    "    return S_db\n",
    "\n",
    "def robust_stats_1d_safe(x):\n",
    "    \"\"\"Stats without NaN.\"\"\"\n",
    "    x = clean_vec(x)\n",
    "    mean_ = float(np.mean(x))\n",
    "    std_  = float(np.std(x))\n",
    "    med   = float(np.median(x))\n",
    "    mad   = float(np.median(np.abs(x - med)))\n",
    "    p10   = float(np.percentile(x, 10))\n",
    "    p90   = float(np.percentile(x, 90))\n",
    "    if x.size < 3 or std_ < 1e-20:\n",
    "        sk = 0.0\n",
    "        ku = 0.0\n",
    "    else:\n",
    "        sk = float(skew(x, bias=False))\n",
    "        ku = float(kurtosis(x, fisher=True, bias=False))\n",
    "        if not np.isfinite(sk): sk = 0.0\n",
    "        if not np.isfinite(ku): ku = 0.0\n",
    "    return dict(mean=mean_, std=std_, median=med, mad=mad, p10=p10, p90=p90, skew=sk, kurt=ku)\n",
    "\n",
    "def summarize_feature(vec, prefix):\n",
    "    stats = robust_stats_1d_safe(vec)\n",
    "    return {f\"{prefix}_{k}\": v for k, v in stats.items()}\n",
    "def extract_librosa_superfeatures(y, sr, fixed_len=16000, n_mfcc=20, n_fft=512, hop=256, win_length=None):\n",
    "    feats = {}\n",
    "\n",
    "    # --- Prepro ---\n",
    "    if y.ndim > 1:\n",
    "        y = librosa.to_mono(y)\n",
    "    if y.size == 0:\n",
    "        y = np.zeros(fixed_len, dtype=np.float32)\n",
    "    y = y - np.mean(y)\n",
    "    rms = np.sqrt(np.mean(y**2) + EPS)\n",
    "    y = y / rms\n",
    "    y = pad_or_trim(y, L=fixed_len)\n",
    "\n",
    "    # --- STFT ---\n",
    "    S = np.abs(librosa.stft(y, n_fft=n_fft, hop_length=hop, win_length=win_length))**2\n",
    "    S_db = power_to_db_safe(S)\n",
    "    feats.update(summarize_feature(S_db, \"spec_db\"))\n",
    "\n",
    "    # --- Mel + flux ---\n",
    "    S_mel = librosa.feature.melspectrogram(y=y, sr=sr, n_fft=n_fft, hop_length=hop, power=2.0)\n",
    "    S_mel_db = power_to_db_safe(S_mel)\n",
    "    flux = librosa.onset.onset_strength(S=S_mel_db, sr=sr, hop_length=hop)\n",
    "    feats.update(summarize_feature(flux, \"flux\"))\n",
    "\n",
    "    # --- Clasic ---\n",
    "    zcr = librosa.feature.zero_crossing_rate(y, frame_length=n_fft, hop_length=hop)[0]\n",
    "    feats.update(summarize_feature(zcr, \"zcr\"))\n",
    "\n",
    "    rms_f = librosa.feature.rms(y=y, frame_length=n_fft, hop_length=hop)[0]\n",
    "    feats.update(summarize_feature(rms_f, \"rms\"))\n",
    "\n",
    "    centroid  = librosa.feature.spectral_centroid(S=S, sr=sr)[0]\n",
    "    bandwidth = librosa.feature.spectral_bandwidth(S=S, sr=sr)[0]\n",
    "    roll85    = librosa.feature.spectral_rolloff(S=S, sr=sr, roll_percent=0.85)[0]\n",
    "    roll95    = librosa.feature.spectral_rolloff(S=S, sr=sr, roll_percent=0.95)[0]\n",
    "    flatness  = librosa.feature.spectral_flatness(S=S)[0]\n",
    "\n",
    "    # clean before update\n",
    "    feats.update(summarize_feature(clean_vec(centroid),  \"centroid\"))\n",
    "    feats.update(summarize_feature(clean_vec(bandwidth), \"bandwidth\"))\n",
    "    feats.update(summarize_feature(clean_vec(roll85),    \"rolloff85\"))\n",
    "    feats.update(summarize_feature(clean_vec(roll95),    \"rolloff95\"))\n",
    "    feats.update(summarize_feature(clean_vec(flatness),  \"flatness\"))\n",
    "\n",
    "    # --- MFCCs + deltas ---\n",
    "    mfcc  = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=n_mfcc, n_fft=n_fft, hop_length=hop)\n",
    "    delta = librosa.feature.delta(mfcc, order=1)\n",
    "    delta2= librosa.feature.delta(mfcc, order=2)\n",
    "    for i in range(n_mfcc):\n",
    "        feats.update(summarize_feature(clean_vec(mfcc[i]),   f\"mfcc{i+1}\"))\n",
    "        feats.update(summarize_feature(clean_vec(delta[i]),  f\"mfcc{i+1}_d1\"))\n",
    "        feats.update(summarize_feature(clean_vec(delta2[i]), f\"mfcc{i+1}_d2\"))\n",
    "\n",
    "    # --- Chroma / Tonnetz  ---\n",
    "    try:\n",
    "        S_lin = np.abs(librosa.stft(y, n_fft=n_fft, hop_length=hop))**2\n",
    "        chroma = librosa.feature.chroma_stft(S=S_lin, sr=sr)\n",
    "        for i in range(chroma.shape[0]):\n",
    "            feats.update(summarize_feature(clean_vec(chroma[i]), f\"chroma{i+1}\"))\n",
    "        feats.update(summarize_feature(clean_vec(chroma), \"chroma_all\"))\n",
    "    except Exception:\n",
    "        feats[\"chroma_all_mean\"] = 0.0\n",
    "\n",
    "    try:\n",
    "        y_harm, _ = librosa.effects.hpss(y)\n",
    "        tonnetz = librosa.feature.tonnetz(y=y_harm, sr=sr)\n",
    "        for i in range(tonnetz.shape[0]):\n",
    "            feats.update(summarize_feature(clean_vec(tonnetz[i]), f\"tonnetz{i+1}\"))\n",
    "        feats.update(summarize_feature(clean_vec(tonnetz), \"tonnetz_all\"))\n",
    "    except Exception:\n",
    "        feats[\"tonnetz_all_mean\"] = 0.0\n",
    "\n",
    "    # --- Mel  ---\n",
    "    feats.update(summarize_feature(S_mel_db, \"mel_db\"))\n",
    "\n",
    "    # --- global mesurements ---\n",
    "    peak = float(np.max(np.abs(y)))\n",
    "    avg  = float(np.mean(np.abs(y))) + EPS\n",
    "    cf   = float(peak / (np.sqrt(np.mean(y**2) + EPS)))\n",
    "    feats.update(dict(\n",
    "        peak=peak,\n",
    "        mean_abs=avg,\n",
    "        peak_to_avg=float(peak / avg),\n",
    "        crest_factor=cf\n",
    "    ))\n",
    "    return feats\n"
   ],
   "id": "dabf38648f24ddf5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.signal import cwt, morlet2, ricker\n",
    "from scipy.stats import entropy, kurtosis\n",
    "\n",
    "\n",
    "def zscore_normalize(y):\n",
    "    return (y - np.mean(y)) / (np.std(y) + 1e-8)\n",
    "\n",
    "def trim_leading_seconds(y, sr, secs=LEAD_TRIM_S):\n",
    "    off = int(round(secs * sr))\n",
    "    return y[off:] if off < len(y) else np.array([], dtype=y.dtype)\n",
    "\n",
    "def pad_or_trim(y, L=FIXED_LEN):\n",
    "    if len(y) < L: y = np.pad(y, (0, L-len(y)), mode=\"constant\")\n",
    "    else:          y = y[:L]\n",
    "    return y.astype(np.float32, copy=False)\n",
    "\n",
    "# ======== Wavelet transforms ========\n",
    "def cwt_morlet_power(y, sr, w=6.0, n_scales=64, min_ms=12.5, max_ms=500.0):\n",
    "    # scale ~ period*w/(2π) -> we choose scales by range of periods\n",
    "\n",
    "    a_min = max(1, int(np.round((min_ms/1000.0) * sr * w / (2*np.pi))))\n",
    "    a_max = int(np.round((max_ms/1000.0) * sr * w / (2*np.pi)))\n",
    "    widths = np.unique(np.linspace(a_min, a_max, n_scales).astype(int))\n",
    "    coef = cwt(y, morlet2, widths, w=w)            # (n_scales, T)\n",
    "    P = np.abs(coef)**2\n",
    "    f_hz = (w * sr) / (2*np.pi*widths)             # Hz aprox \n",
    "    return P, f_hz\n",
    "\n",
    "def cwt_ricker_power(y, sr, n_scales=64, min_ms=4.0, max_ms=200.0):\n",
    "    a_min = max(1, int(np.round((min_ms/1000.0) * sr / 2.0)))\n",
    "    a_max = int(np.round((max_ms/1000.0) * sr / 2.0))\n",
    "    widths = np.unique(np.linspace(a_min, a_max, n_scales).astype(int))\n",
    "    coef = cwt(y, ricker, widths)                  # (n_scales, T)\n",
    "    P = np.abs(coef)**2\n",
    "    f_hz = sr / (2*np.pi*widths)                   # Hz aprox\n",
    "    return P, f_hz\n",
    "\n",
    "# ======== Resúmenes (features) ========\n",
    "# typical bands adjust if required (Hz)\n",
    "WAVE_BANDS_HZ = [(0,80), (80,200), (200,500), (500,1000)]\n",
    "\n",
    "def summarize_cwt(P, f_hz, prefix=\"m\"):\n",
    "    \"\"\"\n",
    "    P: (n_scales, T) power ; f_hz: (n_scales,)\n",
    "    Returns dict with centroid/spread/entropy + band energy ratios + time.\n",
    "    \"\"\"\n",
    "\n",
    "    feats = {}\n",
    "    # ordenar por frecuencia ascendente\n",
    "    order = np.argsort(f_hz)\n",
    "    f = f_hz[order]\n",
    "    S = P[order, :]  # (F,T)\n",
    "\n",
    "    # espectro promedio (sobre tiempo) y energía temporal\n",
    "    Ef = S.mean(axis=1)           # energía por frecuencia promedio\n",
    "    Et = S.sum(axis=0)            # energía por tiempo\n",
    "\n",
    "    tot = S.sum() + 1e-12\n",
    "    feats[f\"{prefix}_power_total\"] = float(tot)\n",
    "\n",
    "    # centroid/spread (en Hz) con espectro promedio Ef\n",
    "    w = Ef / (Ef.sum() + 1e-12)\n",
    "    f_cent = float((w * f).sum())\n",
    "    f_spread = float(np.sqrt((w * (f - f_cent)**2).sum()))\n",
    "    feats[f\"{prefix}_f_centroid_hz\"] = f_cent\n",
    "    feats[f\"{prefix}_f_spread_hz\"]   = f_spread\n",
    "\n",
    "    # entropy (normalizada) del espectro promedio\n",
    "    p = w + 1e-12\n",
    "    H = float(entropy(p, base=np.e) / np.log(len(p)))  # 0..1\n",
    "    feats[f\"{prefix}_f_entropy\"] = H\n",
    "\n",
    "    # band energy ratios\n",
    "    for lo, hi in WAVE_BANDS_HZ:\n",
    "        mask = (f >= lo) & (f < hi)\n",
    "        band = float(S[mask, :].sum() / tot) if mask.any() else 0.0\n",
    "        feats[f\"{prefix}_band_{int(lo)}_{int(hi)}_pct\"] = band\n",
    "\n",
    "    # temporal (transientness)\n",
    "    p50 = float(np.percentile(Et, 50))\n",
    "    p95 = float(np.percentile(Et, 95))\n",
    "    feats[f\"{prefix}_time_contrast\"] = float(p95 / (p50 + 1e-12))\n",
    "    feats[f\"{prefix}_time_kurtosis\"] = float(kurtosis(Et, fisher=True, bias=False))\n",
    "\n",
    "    # estadisticos globales del mapa\n",
    "    feats[f\"{prefix}_map_mean\"] = float(S.mean())\n",
    "    feats[f\"{prefix}_map_std\"]  = float(S.std())\n",
    "    feats[f\"{prefix}_map_skew\"] = float(((S - S.mean())**3).mean() / (S.std()**3 + 1e-12))\n",
    "    feats[f\"{prefix}_map_kurt\"] = float(kurtosis(S.flatten(), fisher=True, bias=False))\n",
    "\n",
    "    return feats\n",
    "\n",
    "def extract_wavelet_features_from_row(row,LEAD_TRIM_S = 0.6,FIXED_LEN = 16000):\n",
    "    \"\"\"\n",
    "    row: a row from df_audio with 'data', 'sample_rate', and metadata.\n",
    "    Returns dict with metadata + Morlet and Ricker features.\n",
    "    \"\"\"\n",
    "\n",
    "    y_raw = row[\"data\"]\n",
    "    sr = int(row[\"sample_rate\"])\n",
    "    # prepro igual que en visualización\n",
    "    y = pad_or_trim(zscore_normalize(trim_leading_seconds(y_raw, sr, LEAD_TRIM_S)), L=FIXED_LEN)\n",
    "\n",
    "    # Morlet\n",
    "    Pm, fm = cwt_morlet_power(y, sr, w=6.0, n_scales=128, min_ms=12.5, max_ms=500.0)\n",
    "    feats_m = summarize_cwt(Pm, fm, prefix=\"morlet\")\n",
    "\n",
    "    # Ricker (transitorios)\n",
    "    Pr, fr = cwt_ricker_power(y, sr, n_scales=128, min_ms=4.0, max_ms=200.0)\n",
    "    feats_r = summarize_cwt(Pr, fr, prefix=\"ricker\")\n",
    "\n",
    "    # metadatos para merge\n",
    "    return {\n",
    "        \"filename\": str(row.get(\"filename\",\"\")),\n",
    "        \"patient_id\": row.get(\"patient_id\", None),\n",
    "        \"label\": str(row.get(\"label\",\"\")),\n",
    "        \"mode\": str(row.get(\"filter_code\", row.get(\"mode\",\"\"))),\n",
    "        **feats_m,\n",
    "        **feats_r,\n",
    "    }\n"
   ],
   "id": "cfff3f6f26758d0a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##Fraiwan et al features##",
   "id": "ba54baaaa21ea767"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "# === Fraiwan et al. 2020 — Entropy features ===\n",
    "# - Shannon entropy (time): histogram with Scott’s bandwidth; p = hist/sum(hist); ShaEn = -sum p log p\n",
    "# - Log-energy entropy: LogEn = -sum (log p)^2   (over the same amplitude histogram)\n",
    "# - Spectral entropy (from power spectrogram): S(t,f) -> P(f) = sum_t S / sum_{f,t} S; SpeEn = -sum P log P\n",
    "# - Dataset-wise min-max [0,1] per column\n",
    "#\n",
    "# For consistency with the paper:\n",
    "#   sr_target = 4000 Hz\n",
    "#   dur_target_s = 5.0  →  n_target = 20,000 samples\n",
    "#\n",
    "# Expected inputs:\n",
    "#   - df_audio with columns: [\"data\", \"sample_rate\", \"filename\", \"patient_id\", \"label\", \"mode\" (or filter_code)]\n",
    "#\n",
    "# Outputs:\n",
    "#   - df_feats_fraiwan with: [\"filename\",\"patient_id\",\"label\",\"mode\",\"sha_entropy\",\"log_energy_entropy\",\"spec_entropy\"]\n",
    "#   - df_feats_clasic_spectral_entropy = merge with your already existing features\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import librosa\n",
    "from pathlib import Path\n",
    "\n",
    "# ----------------------------\n",
    "# Parameters\n",
    "# ----------------------------\n",
    "SR_TARGET = 4000\n",
    "DUR_S     = 5.0\n",
    "N_TARGET  = int(SR_TARGET * DUR_S)\n",
    "\n",
    "# Espectrograma: STFT\n",
    "N_FFT     = 512\n",
    "HOP       = 128\n",
    "WIN       = \"hann\"\n",
    "FMIN      = 0\n",
    "FMAX      = None  # a SR_TARGET/2\n",
    "\n",
    "# ----------------------------\n",
    "# Helpers\n",
    "# ----------------------------\n",
    "def _resample_pad_trim(y, sr, sr_target=SR_TARGET, n_target=N_TARGET):\n",
    "    \"\"\"Resample a sr_target y recorta/paddea a n_target.\"\"\"\n",
    "    if sr != sr_target:\n",
    "        y = librosa.resample(y.astype(np.float32, copy=False), orig_sr=sr, target_sr=sr_target)\n",
    "    if len(y) < n_target:\n",
    "        y = np.pad(y, (0, n_target-len(y)), mode=\"constant\")\n",
    "    else:\n",
    "        y = y[:n_target]\n",
    "    return y.astype(np.float32, copy=False)\n",
    "\n",
    "def _scott_bin_edges(x: np.ndarray):\n",
    "    \"\"\"Regla de Scott para ancho de bin; si var≈0, usa bins fijos.\"\"\"\n",
    "    x = np.asarray(x, dtype=np.float64)\n",
    "    n  = x.size\n",
    "    sd = np.std(x)\n",
    "    if n < 2 or sd < 1e-12:\n",
    "        # caso degenerado: 10 bins alrededor del valor medio\n",
    "        m = float(np.mean(x)) if n > 0 else 0.0\n",
    "        w = 1e-3 if sd < 1e-12 else 3.49*sd*(n**(-1/3))\n",
    "        return np.linspace(m-5*w, m+5*w, 11)\n",
    "    width = 3.49 * sd * (n ** (-1/3))\n",
    "    nbins = int(np.ceil((x.max() - x.min()) / (width + 1e-12)))\n",
    "    nbins = max(10, min(512, nbins))  # acotar\n",
    "    return np.linspace(x.min(), x.max(), nbins+1)\n",
    "\n",
    "def shannon_entropy_from_hist(p: np.ndarray):\n",
    "    p = p[p > 0]\n",
    "    return float(-(p * np.log(p)).sum())\n",
    "\n",
    "import numpy as np\n",
    "import librosa\n",
    "from scipy.stats import entropy\n",
    "\n",
    "# --- helpers nuevos ---\n",
    "def _safe_entropy(p):\n",
    "    p = p[p > 0]\n",
    "    if p.size == 0:\n",
    "        return 0.0\n",
    "    H = float(-(p * np.log(p)).sum())\n",
    "    # normaliza por log(k) para quedar en [0,1]\n",
    "    return H / float(np.log(len(p)))\n",
    "\n",
    "def _safe_log_energy_entropy(p):\n",
    "    p = p[p > 0]\n",
    "    if p.size == 0:\n",
    "        return 0.0\n",
    "    H = float(-np.sum((np.log(p))**2))\n",
    "    # opcional: normaliza por (log(k))**2\n",
    "    return H / float(np.log(len(p))**2 + 1e-12)\n",
    "\n",
    "def _resample_pad_trim(y, sr, sr_target=SR_TARGET, n_target=N_TARGET):\n",
    "    if sr != sr_target:\n",
    "        y = librosa.resample(y.astype(np.float32, copy=False),\n",
    "                             orig_sr=sr, target_sr=sr_target, res_type=\"kaiser_fast\")\n",
    "    if len(y) < n_target:\n",
    "        y = np.pad(y, (0, n_target-len(y)), mode=\"constant\")\n",
    "    else:\n",
    "        y = y[:n_target]\n",
    "    return y.astype(np.float32, copy=False)\n",
    "\n",
    "def spectral_entropy_from_S(S_power, sr=SR_TARGET, fmin=20, fmax=None):\n",
    "    S_power = np.asarray(S_power, dtype=np.float64)\n",
    "    if S_power.ndim != 2 or S_power.size == 0:\n",
    "        return 0.0\n",
    "    n_fft = (S_power.shape[0]-1)*2\n",
    "    freqs = np.linspace(0, sr/2, S_power.shape[0])\n",
    "    mask = (freqs >= (fmin or 0)) & (freqs <= (fmax or (sr/2)))\n",
    "    if not np.any(mask):\n",
    "        return 0.0\n",
    "    Ef = S_power[mask, :].sum(axis=1)\n",
    "    tot = Ef.sum()\n",
    "    if tot <= 0:\n",
    "        return 0.0\n",
    "    p = Ef / tot\n",
    "    p = p[p > 0]\n",
    "    # entropia normalizada\n",
    "    return float(-(p * np.log(p)).sum() / np.log(len(p)))\n",
    "\n",
    "def compute_fraiwan_entropies(y: np.ndarray, sr: int):\n",
    "    # 1) Resample + 5s @ 4kHz\n",
    "    y = _resample_pad_trim(y, sr, SR_TARGET, N_TARGET)\n",
    "\n",
    "    # 2) Opcional: normalizar amplitud para ShaEn/LogEn\n",
    "    y_amp = y / (np.sqrt(np.mean(y**2) + 1e-12))\n",
    "\n",
    "    # Histograma (Scott) sobre y_amp\n",
    "    edges = _scott_bin_edges(y_amp)\n",
    "    hist, _ = np.histogram(y_amp, bins=edges)\n",
    "    p = hist.astype(np.float64)\n",
    "    p = p / (p.sum() + 1e-12)\n",
    "\n",
    "    # 3) Entropías de tiempo (normalizadas)\n",
    "    sha  = _safe_entropy(p)\n",
    "    loge = _safe_log_energy_entropy(p)\n",
    "\n",
    "    # 4) Entropía espectral (normalizada) con máscara de frecuencias\n",
    "    S = librosa.stft(y, n_fft=N_FFT, hop_length=HOP, window=WIN, center=True)\n",
    "    S_power = np.abs(S)**2\n",
    "    spec = spectral_entropy_from_S(S_power, sr=SR_TARGET, fmin=20, fmax=1000)\n",
    "\n",
    "    return sha, loge, spec\n",
    "\n"
   ],
   "id": "a76bcec2990a801",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Fraiwan Features ##",
   "id": "c1bfa11af4ad158c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Feature caching & extractor wrappers ##",
   "id": "c928dd6a655e5dc9"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "from __future__ import annotations\n",
    "from pathlib import Path\n",
    "import hashlib\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ============================\n",
    "# Config\n",
    "# ============================\n",
    "CACHE_DIR = Path(\"./features_cache\")\n",
    "CACHE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "KEYS = [\"filename\",\"patient_id\",\"label\",\"mode\"]  # claves de merge consistentes\n",
    "\n",
    "# ============================\n",
    "# Utils\n",
    "# ============================\n",
    "def _norm_keys(df: pd.DataFrame, keys=KEYS) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    for c in keys:\n",
    "        if c in df.columns:\n",
    "            df[c] = df[c].astype(str).str.strip()\n",
    "        else:\n",
    "            # crea si falta, para evitar fallos de merge\n",
    "            df[c] = \"\" if c != \"patient_id\" else None\n",
    "    return df\n",
    "\n",
    "def _hash_params(params: dict) -> str:\n",
    "    s = json.dumps(params, sort_keys=True, default=str)\n",
    "    return hashlib.md5(s.encode()).hexdigest()[:8]\n",
    "\n",
    "def _save_parquet(df: pd.DataFrame, path: Path):\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    df.to_parquet(path, index=False)\n",
    "\n",
    "def _load_parquet(path: Path) -> pd.DataFrame:\n",
    "    return pd.read_parquet(path)\n",
    "\n",
    "def _exists_ok(path: Path, force: bool) -> bool:\n",
    "    return path.exists() and not force\n",
    "\n",
    "def _sanitize(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    return (df.replace([np.inf, -np.inf], np.nan).fillna(0.0))\n",
    "\n",
    "# ============================\n",
    "# Extractores por fila \n",
    "# ============================\n",
    "\n",
    "# - extract_classic_features(y, sr, lead_trim_s, fixed_len)\n",
    "# - extract_librosa_superfeatures(y, sr, fixed_len, n_mfcc, n_fft, hop)\n",
    "# - extract_wavelet_features_from_row(row)   # ya recibe fila y arma dict\n",
    "# - compute_fraiwan_entropies(y, sr)         # devuelve (sha, loge, spec)\n",
    "\n",
    "# ============================\n",
    "# BUILDERS (df_audio -> df_features, con cache)\n",
    "# ============================\n",
    "def build_classic_df(df_audio: pd.DataFrame,\n",
    "                     lead_trim_s: float = 0.6,\n",
    "                     fixed_len: int = 16000,\n",
    "                     cache_dir: Path = CACHE_DIR,\n",
    "                     force: bool = False) -> pd.DataFrame:\n",
    "    params = dict(kind=\"classic\", lead_trim_s=lead_trim_s, fixed_len=fixed_len)\n",
    "    h = _hash_params(params)\n",
    "    path = cache_dir / f\"classic_{h}.parquet\"\n",
    "    if _exists_ok(path, force):\n",
    "        df = _load_parquet(path)\n",
    "        return _sanitize(_norm_keys(df))\n",
    "    # compute\n",
    "    rows = []\n",
    "    for i, r in df_audio.iterrows():\n",
    "        y  = r[\"data\"]; sr = int(r[\"sample_rate\"])\n",
    "        feats = extract_classic_features(y, sr, lead_trim_s=lead_trim_s, fixed_len=fixed_len)\n",
    "        feats.update(dict(\n",
    "            filename=str(r.get(\"filename\",\"\")),\n",
    "            patient_id=r.get(\"patient_id\", None),\n",
    "            label=str(r.get(\"label\",\"\")),\n",
    "            mode=str(r.get(\"filter_code\", r.get(\"mode\",\"\"))),\n",
    "            sample_rate=sr,\n",
    "            idx=i,\n",
    "        ))\n",
    "        rows.append(feats)\n",
    "    df = pd.DataFrame(rows)\n",
    "    df = _sanitize(_norm_keys(df))\n",
    "    _save_parquet(df, path)\n",
    "    return df\n",
    "\n",
    "def build_librosa_df(df_audio: pd.DataFrame,\n",
    "                     fixed_len: int = 16000,\n",
    "                     n_mfcc: int = 20,\n",
    "                     n_fft: int = 512,\n",
    "                     hop: int = 256,\n",
    "                     cache_dir: Path = CACHE_DIR,\n",
    "                     force: bool = False) -> pd.DataFrame:\n",
    "    params = dict(kind=\"librosa\", fixed_len=fixed_len, n_mfcc=n_mfcc, n_fft=n_fft, hop=hop)\n",
    "    h = _hash_params(params)\n",
    "    path = cache_dir / f\"librosa_{h}.parquet\"\n",
    "    if _exists_ok(path, force):\n",
    "        df = _load_parquet(path)\n",
    "        return _sanitize(_norm_keys(df))\n",
    "    rows = []\n",
    "    for i, r in df_audio.iterrows():\n",
    "        y  = r[\"data\"]; sr = int(r[\"sample_rate\"])\n",
    "        f  = extract_librosa_superfeatures(y, sr, fixed_len=fixed_len, n_mfcc=n_mfcc, n_fft=n_fft, hop=hop)\n",
    "        f.update(dict(\n",
    "            idx=i,\n",
    "            filename=str(r.get(\"filename\",\"\")),\n",
    "            patient_id=r.get(\"patient_id\", None),\n",
    "            label=str(r.get(\"label\",\"\")),\n",
    "            mode=str(r.get(\"filter_code\", r.get(\"mode\",\"\"))),\n",
    "            sample_rate=sr,\n",
    "        ))\n",
    "        rows.append(f)\n",
    "    df = pd.DataFrame(rows)\n",
    "    df = _sanitize(_norm_keys(df))\n",
    "    _save_parquet(df, path)\n",
    "    return df\n",
    "\n",
    "def build_wavelet_df(df_audio: pd.DataFrame,\n",
    "                     cache_dir: Path = CACHE_DIR,\n",
    "                     force: bool = False,\n",
    "                     prefix: str = \"wavelet\") -> pd.DataFrame:\n",
    "    params = dict(kind=\"wavelet\")\n",
    "    h = _hash_params(params)\n",
    "    path = cache_dir / f\"{prefix}_{h}.parquet\"\n",
    "    if _exists_ok(path, force):\n",
    "        df = _load_parquet(path)\n",
    "        return _sanitize(_norm_keys(df))\n",
    "    rows = []\n",
    "    for _, r in df_audio.iterrows():\n",
    "        try:\n",
    "            rows.append(extract_wavelet_features_from_row(r))\n",
    "        except Exception as e:\n",
    "            print(f\"[wavelet WARN] {r.get('filename','?')}: {e}\")\n",
    "    df = pd.DataFrame(rows)\n",
    "    df = _sanitize(_norm_keys(df))\n",
    "    _save_parquet(df, path)\n",
    "    return df\n",
    "\n",
    "def build_fraiwan_df(df_audio: pd.DataFrame,\n",
    "                     cache_dir: Path = CACHE_DIR,\n",
    "                     force: bool = False) -> pd.DataFrame:\n",
    "    params = dict(kind=\"fraiwan\")\n",
    "    h = _hash_params(params)\n",
    "    path = cache_dir / f\"fraiwan_{h}.parquet\"\n",
    "    if _exists_ok(path, force):\n",
    "        df = _load_parquet(path)\n",
    "        return _sanitize(_norm_keys(df))\n",
    "    rows = []\n",
    "    for _, r in df_audio.iterrows():\n",
    "        try:\n",
    "            y = r[\"data\"]; sr = int(r.get(\"sample_rate\", 4000))\n",
    "            sha, loge, spec = compute_fraiwan_entropies(y, sr)\n",
    "            rows.append({\n",
    "                \"filename\":   str(r.get(\"filename\",\"\")),\n",
    "                \"patient_id\": r.get(\"patient_id\", None),\n",
    "                \"label\":      str(r.get(\"label\",\"\")),\n",
    "                \"mode\":       str(r.get(\"filter_code\", r.get(\"mode\",\"\"))),\n",
    "                \"sha_entropy\": sha,\n",
    "                \"log_energy_entropy\": loge,\n",
    "                \"spec_entropy\": spec,\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(f\"[fraiwan WARN] {r.get('filename','?')}: {e}\")\n",
    "    df = pd.DataFrame(rows)\n",
    "    df = _sanitize(_norm_keys(df))\n",
    "    _save_parquet(df, path)\n",
    "    return df\n",
    "\n",
    "# ============================\n",
    "# MERGE y ORQUESTADOR\n",
    "# ============================\n",
    "def merge_feature_dfs(dfs: list[pd.DataFrame], keys=KEYS) -> pd.DataFrame:\n",
    "    assert len(dfs) >= 1\n",
    "    base = _norm_keys(dfs[0])\n",
    "    for d in dfs[1:]:\n",
    "        base = base.merge(_norm_keys(d), on=keys, how=\"inner\")\n",
    "    return _sanitize(base)\n",
    "\n",
    "def extract_all_features(df_audio: pd.DataFrame,\n",
    "                         cache_dir: Path = CACHE_DIR,\n",
    "                         force: bool = False) -> dict[str, pd.DataFrame]:\n",
    "    # 1) construir cada set (si no existen)\n",
    "    df_cla = build_classic_df(df_audio, cache_dir=cache_dir, force=force)\n",
    "    df_lib = build_librosa_df(df_audio, cache_dir=cache_dir, force=force)\n",
    "    df_wav = build_wavelet_df(df_audio, cache_dir=cache_dir, force=force)\n",
    "    df_fra = build_fraiwan_df(df_audio, cache_dir=cache_dir, force=force)\n",
    "\n",
    "    # 2) merges por claves consistentes\n",
    "    df_cla_lib = merge_feature_dfs([df_cla, df_lib])\n",
    "    df_full     = merge_feature_dfs([df_cla_lib, df_wav, df_fra])\n",
    "\n",
    "    # 3) sanity final\n",
    "    bad_cols = df_full.columns[df_full.isna().any()]\n",
    "    if len(bad_cols):\n",
    "        print(\"[WARN] NaNs detectados en columnas:\", list(bad_cols))\n",
    "        df_full = _sanitize(df_full)\n",
    "\n",
    "    return dict(\n",
    "        classic=df_cla,\n",
    "        librosa=df_lib,\n",
    "        wavelet=df_wav,\n",
    "        fraiwan=df_fra,\n",
    "        full=df_full\n",
    "    )\n"
   ],
   "id": "cc3b0ffafd6adb61",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "outs = extract_all_features(df_audio, cache_dir=Path(\"./features_cache\"), force=False)\n",
    "\n",
    "df_feats_clasic   = outs[\"classic\"]\n",
    "df_feats_spectral = outs[\"librosa\"]\n",
    "df_feats_wavelet  = outs[\"wavelet\"]\n",
    "df_feats_fraiwan  = outs[\"fraiwan\"]\n",
    "df_feats_full     = outs[\"full\"]\n",
    "\n",
    "print(\"classic:\", df_feats_clasic.shape)\n",
    "print(\"librosa:\", df_feats_spectral.shape)\n",
    "print(\"wavelet:\", df_feats_wavelet.shape)\n",
    "print(\"fraiwan:\", df_feats_fraiwan.shape)\n",
    "print(\"FULL:\", df_feats_full.shape)\n"
   ],
   "id": "415ee369aebef46f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import List, Optional, Dict, Any, Tuple\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# UMAP import with fallback\n",
    "try:\n",
    "    import umap.umap_ as umap\n",
    "except Exception:\n",
    "    import umap  # in case your installation exposes UMAP here\n",
    "\n",
    "\n",
    "def umap_per_patient(\n",
    "    df_feats: pd.DataFrame,\n",
    "    title: str = \"UMAP per patient (concat EP|DP|BP with padding)\",\n",
    "    mode_order: List[str] = [\"EP\", \"DP\", \"BP\"],\n",
    "    meta_cols: List[str] = (\"filename\", \"label\", \"patient_id\", \"mode\", \"mode_clasic\",\"idx\",\"sample_rate\"),\n",
    "    keys_for_merge: List[str] = (\"patient_id\",),  # keys for merging with extra_merge\n",
    "    extra_merge: Optional[pd.DataFrame] = None,   # optional DF to join at patient level\n",
    "    # UMAP & scaling\n",
    "    n_neighbors: int = 10,\n",
    "    min_dist: float = 0.1,\n",
    "    metric: str = \"cosine\",\n",
    "    random_state: int = 42,\n",
    "    do_plot: bool = True,\n",
    "    figsize: Tuple[int, int] = (8, 6),\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Builds one vector per patient by concatenating EP|DP|BP features (with padding)\n",
    "    and projects with UMAP. Optionally joins an extra DF at the 'patient_id' level.\n",
    "\n",
    "    Parameters:\n",
    "    - df_feats: DataFrame with at least these meta columns:\n",
    "                ['filename','label','patient_id','mode' or 'mode_clasic'].\n",
    "                All other columns will be treated as features.\n",
    "    - title: plot title.\n",
    "    - mode_order: concatenation order of modes.\n",
    "    - meta_cols: meta columns (not used as features).\n",
    "    - keys_for_merge: keys to merge with extra_merge (default, 'patient_id').\n",
    "    - extra_merge: optional DF to join to the final result by the provided keys.\n",
    "    - n_neighbors, min_dist, metric, random_state: UMAP hyperparameters.\n",
    "    - do_plot: if True, draws a scatter colored by the patient's majority label.\n",
    "    - figsize: figure size.\n",
    "\n",
    "    Returns:\n",
    "    dict with:\n",
    "      - df_pat: final per-patient DataFrame with 'umap_x','umap_y' (+ extra_merge if provided)\n",
    "      - scaler: StandardScaler fitted on the concatenated columns\n",
    "      - umap_model: fitted UMAP model\n",
    "      - colnames_concat: list of concatenated feature column names\n",
    "      - patient_label: DataFrame with the majority label per patient\n",
    "    \"\"\"\n",
    "    df = df_feats.copy()\n",
    "\n",
    "    # --- key coercion and cleaning ---\n",
    "    for c in [\"patient_id\", \"label\"]:\n",
    "        if c in df.columns:\n",
    "            df[c] = df[c].astype(str).str.strip()\n",
    "    # canonicalize 'mode'\n",
    "    if \"mode\" not in df.columns or df[\"mode\"].isna().all():\n",
    "        if \"mode_clasic\" in df.columns:\n",
    "            df[\"mode\"] = df[\"mode_clasic\"]\n",
    "        else:\n",
    "            raise ValueError(\"Couldn't find 'mode' or 'mode_clasic' column in df_feats.\")\n",
    "    df[\"mode\"] = df[\"mode\"].astype(str).str.strip()\n",
    "\n",
    "    # --- define feature columns ---\n",
    "    feat_cols = [c for c in df.columns if c not in meta_cols and c != \"mode\"]\n",
    "\n",
    "    if len(feat_cols) == 0:\n",
    "        raise ValueError(\"No feature columns remain after excluding meta_cols.\")\n",
    "\n",
    "    # --- aggregate by (patient_id, mode): mean if there are multiple records ---\n",
    "    agg = (\n",
    "        df.groupby([\"patient_id\", \"mode\"], as_index=False)[feat_cols]\n",
    "          .mean()\n",
    "    )\n",
    "\n",
    "    # --- majority label per patient ---\n",
    "    def _majority_label(g):\n",
    "        return g[\"label\"].mode().iloc[0] if len(g) else \"Unknown\"\n",
    "\n",
    "    patient_label = (\n",
    "        df.groupby(\"patient_id\")\n",
    "          .apply(_majority_label)\n",
    "          .rename(\"patient_label\")\n",
    "          .reset_index()\n",
    "    )\n",
    "\n",
    "    # --- patient × (features_EP + features_DP + features_BP) matrix with padding ---\n",
    "    rows = []\n",
    "    pids = sorted(df[\"patient_id\"].dropna().unique())\n",
    "\n",
    "    for pid in pids:\n",
    "        vecs = []\n",
    "        for m in mode_order:\n",
    "            row = agg[(agg[\"patient_id\"] == pid) & (agg[\"mode\"] == m)]\n",
    "            if len(row) == 1:\n",
    "                v = row[feat_cols].to_numpy(dtype=float).ravel()\n",
    "            else:\n",
    "                v = np.zeros(len(feat_cols), dtype=float)  # padding if mode is missing\n",
    "            vecs.append(v)\n",
    "        concat_vec = np.concatenate(vecs, axis=0)\n",
    "        rows.append({\"patient_id\": pid, \"features\": concat_vec})\n",
    "\n",
    "    df_pat = pd.DataFrame(rows)\n",
    "    colnames_concat = [f\"{f}__{m}\" for m in mode_order for f in feat_cols]\n",
    "\n",
    "    X_pat = np.vstack(df_pat[\"features\"].to_numpy())\n",
    "    df_pat_feat = pd.DataFrame(X_pat, columns=colnames_concat)\n",
    "    df_pat = pd.concat([df_pat[[\"patient_id\"]].reset_index(drop=True), df_pat_feat], axis=1)\n",
    "\n",
    "    # add majority label\n",
    "    df_pat = df_pat.merge(patient_label, on=\"patient_id\", how=\"left\")\n",
    "\n",
    "    # --- column-wise normalization ---\n",
    "    scaler = StandardScaler()\n",
    "    X_norm = scaler.fit_transform(df_pat[colnames_concat].to_numpy(dtype=float))\n",
    "    \n",
    "    # --- UMAP ---\n",
    "    umap_model = umap.UMAP(\n",
    "        n_neighbors=n_neighbors,\n",
    "        min_dist=min_dist,\n",
    "        metric=metric,\n",
    "        random_state=random_state\n",
    "    )\n",
    "    X_umap = umap_model.fit_transform(X_norm)\n",
    "    df_pat[\"umap_x\"] = X_umap[:, 0]\n",
    "    df_pat[\"umap_y\"] = X_umap[:, 1]\n",
    "\n",
    "    # --- optional merge with extra_merge ---\n",
    "    if extra_merge is not None:\n",
    "        df_pat = df_pat.merge(extra_merge, on=list(keys_for_merge), how=\"left\")\n",
    "\n",
    "    # --- plot ---\n",
    "    if do_plot:\n",
    "        plt.figure(figsize=figsize)\n",
    "        sns.scatterplot(\n",
    "            data=df_pat,\n",
    "            x=\"umap_x\", y=\"umap_y\",\n",
    "            hue=\"patient_label\",\n",
    "            s=60, palette=\"tab10\", edgecolor=\"k\", linewidth=0.3\n",
    "        )\n",
    "        plt.title(title)\n",
    "        plt.xlabel(\"umap_x\"); plt.ylabel(\"umap_y\")\n",
    "        plt.grid(True, alpha=0.25)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    return {\n",
    "        \"df_pat\": df_pat,\n",
    "        \"scaler\": scaler,\n",
    "        \"umap_model\": umap_model,\n",
    "        \"colnames_concat\": colnames_concat,\n",
    "        \"patient_label\": patient_label,\n",
    "    }\n"
   ],
   "id": "22b9dc7577024a1b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "\n",
    "umap_per_patient(\n",
    "    df_feats=df_feats_clasic,\n",
    "    title=\"UMAP per patient - Classic Features\",\n",
    "    # extra_merge=df_alguna_tabla_por_paciente,  # opcional\n",
    ")\n",
    "\n",
    "umap_per_patient(\n",
    "    df_feats=df_feats_spectral,\n",
    "    title=\"UMAP per patient - Spectral\",\n",
    "    # extra_merge=df_alguna_tabla_por_paciente,  # opcional\n",
    ")\n",
    "\n",
    "umap_per_patient(\n",
    "    df_feats=df_feats_wavelet,\n",
    "    title=\"UMAP per patient - wavelets\",\n",
    "    # extra_merge=df_alguna_tabla_por_paciente,  # opcional\n",
    ")\n",
    "\n",
    "umap_per_patient(\n",
    "    df_feats=df_feats_fraiwan,\n",
    "    title=\"UMAP per patient - Frawin\",\n",
    "    # extra_merge=df_alguna_tabla_por_paciente,  # opcional\n",
    ")\n",
    "\n",
    "umap_per_patient(\n",
    "    df_feats=df_feats_full,\n",
    "    title=\"UMAP per patient - Classic+Spectral+wavelets+Farwin\",\n",
    "    # extra_merge=df_alguna_tabla_por_paciente,  # opcional\n",
    ")\n",
    "\n"
   ],
   "id": "b709f5dedf844645",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "# Claves/metadata que quieres preservar en el DF final\n",
    "META_COLS = [\"filename\",\"patient_id\",\"label\",\"mode\",\"idx\",\"sample_rate\",\"n_samples\",\"lead_trim_s\",\"fixed_len\"]\n",
    "\n",
    "def reduce_features_unsupervised(df: pd.DataFrame,\n",
    "                                 meta_cols=META_COLS,\n",
    "                                 var_threshold: float = 1e-5,\n",
    "                                 corr_threshold: float = 0.90,\n",
    "                                 fillna_value: float = 0.0):\n",
    "    \"\"\"\n",
    "    Aplica:\n",
    "      1) VarianceThreshold (descarta casi constantes)\n",
    "      2) Drop por alta correlación (Pearson |r| > corr_threshold)\n",
    "    Devuelve:\n",
    "      - df_reduced: DataFrame coherente = [meta_cols + features_filtradas]\n",
    "      - report: dict con columnas eliminadas en cada etapa y las conservadas\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "\n",
    "    # 0) Asegura que metas existan (si no, crea vacías)\n",
    "    for c in meta_cols:\n",
    "        if c not in df.columns:\n",
    "            df[c] = np.nan\n",
    "\n",
    "    # 1) Seleccionar solo numéricas como candidatas a features\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    # Evitar tocar metadatos numéricos (ej. sample_rate)\n",
    "    feature_cols = [c for c in numeric_cols if c not in meta_cols]\n",
    "\n",
    "    X = df[feature_cols].replace([np.inf, -np.inf], np.nan).fillna(fillna_value)\n",
    "\n",
    "    # 2) VarianceThreshold\n",
    "    vt = VarianceThreshold(threshold=var_threshold)\n",
    "    X_vt = vt.fit_transform(X)\n",
    "    vt_keep = [col for col, keep in zip(feature_cols, vt.get_support()) if keep]\n",
    "    vt_drop = [c for c in feature_cols if c not in vt_keep]\n",
    "\n",
    "    # 3) Drop por correlación (sobre las que pasaron VT)\n",
    "    if len(vt_keep) > 1:\n",
    "        X_corr = pd.DataFrame(X_vt, columns=vt_keep, index=df.index)\n",
    "        corr = X_corr.corr().abs()\n",
    "        upper = corr.where(np.triu(np.ones(corr.shape), k=1).astype(bool))\n",
    "        corr_drop = [col for col in upper.columns if any(upper[col] > corr_threshold)]\n",
    "        corr_keep = [c for c in vt_keep if c not in corr_drop]\n",
    "    else:\n",
    "        corr_drop = []\n",
    "        corr_keep = vt_keep\n",
    "\n",
    "    # 4) Reconstruir DF coherente: metas + features finales\n",
    "    kept_cols = meta_cols + corr_keep\n",
    "    # Mantén sólo columnas que existen realmente (por si alguna meta no estaba)\n",
    "    kept_cols = [c for c in kept_cols if c in df.columns or c in corr_keep]\n",
    "\n",
    "    df_reduced = pd.concat([df[meta_cols], X[corr_keep]], axis=1)\n",
    "\n",
    "    # 5) Reporte\n",
    "    report = dict(\n",
    "        total_features=len(feature_cols),\n",
    "        removed_by_variance=len(vt_drop),\n",
    "        removed_by_correlation=len(corr_drop),\n",
    "        kept=len(corr_keep),\n",
    "        dropped_variance=vt_drop,\n",
    "        dropped_correlation=corr_drop,\n",
    "        kept_features=corr_keep\n",
    "    )\n",
    "\n",
    "    return df_reduced, report\n"
   ],
   "id": "26ac0f5c041783a8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "df_coherent, rep = reduce_features_unsupervised(\n",
    "    df_feats_full,\n",
    "    meta_cols=[\"filename\",\"patient_id\",\"label\",\"mode\",\"idx\",\"sample_rate\"],\n",
    "    var_threshold=1e-5,\n",
    "    corr_threshold=0.90\n",
    ")\n",
    "\n",
    "print(rep[\"total_features\"], \"features totales\")\n",
    "print(rep[\"removed_by_variance\"], \"eliminadas por varianza baja\")\n",
    "print(rep[\"removed_by_correlation\"], \"eliminadas por colinealidad\")\n",
    "print(rep[\"kept\"], \"features finales\")\n",
    "print(\"Primeras 10 kept:\", rep[\"kept_features\"][:10])\n",
    "\n",
    "# (opcional) guardar\n",
    "# df_coherent.to_parquet(\"features_filtered.parquet\", index=False)\n"
   ],
   "id": "ba4a1f23cd99c136",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "umap_per_patient(\n",
    "    df_feats=df_coherent,\n",
    "    title=\"UMAP per patient - Classic+Spectral+wavelets+Farwin\",\n",
    "    # extra_merge=df_alguna_tabla_por_paciente,  # opcional\n",
    ")\n"
   ],
   "id": "28e80924d3ab5486",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Third Part ##",
   "id": "10f81986e157ac6c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "** 0. To Track Classification Experiments**",
   "id": "6446ac70c449c540"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "# !pip install mlflow\n",
    "import mlflow, os, json, glob\n",
    "from pathlib import Path\n",
    "mlflow.set_tracking_uri(\"file:./mlruns\")\n",
    "mlflow.set_experiment(\"samay-audio\")\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report, ConfusionMatrixDisplay\n",
    "\n",
    "def _save_cm_png(cm, labels, title, fname):\n",
    "    fig, ax = plt.subplots(figsize=(5,4))\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)\n",
    "    disp.plot(ax=ax, cmap=plt.cm.Blues, xticks_rotation=45, colorbar=False)\n",
    "    ax.set_title(title)\n",
    "    plt.tight_layout()\n",
    "    fig.savefig(fname, dpi=160, bbox_inches=\"tight\")\n",
    "    plt.close(fig)\n",
    "\n",
    "def mlflow_log_classif(y_true_str, y_pred_str, y_true_enc, y_pred_enc, labels_order, labels_names,\n",
    "                       run_params: dict, model_tag: str, file_prefix: str):\n",
    "    \"\"\"\n",
    "    y_true_str / y_pred_str: etiquetas originales (strings) para el classification_report\n",
    "    y_true_enc / y_pred_enc: etiquetas codificadas (enteros) para confusion_matrix\n",
    "    labels_order: np.arange(n_clases)\n",
    "    labels_names: lista con nombres de clases (para ejes)\n",
    "    run_params: dict de hiperparámetros/nota del run\n",
    "    model_tag: 'KNN' | 'LSTM' | 'RF' | etc\n",
    "    file_prefix: prefijo de archivos ('knn_exp1', 'rf_full', etc)\n",
    "    \"\"\"\n",
    "    # 1) params\n",
    "    for k, v in (run_params or {}).items():\n",
    "        mlflow.log_param(k, v)\n",
    "\n",
    "    # 2) métricas\n",
    "    rep = classification_report(y_true_str, y_pred_str, zero_division=0, output_dict=True)\n",
    "    acc = float(rep.get(\"accuracy\", 0.0))\n",
    "    macro_f1 = float(rep.get(\"macro avg\", {}).get(\"f1-score\", 0.0))\n",
    "    mlflow.log_metric(\"accuracy\", acc)\n",
    "    mlflow.log_metric(\"macro_f1\", macro_f1)\n",
    "\n",
    "    # 3) classification_report -> CSV\n",
    "    df_rep = pd.DataFrame(rep).T.reset_index().rename(columns={\"index\":\"class\"})\n",
    "    csv_path = f\"{file_prefix}_class_report.csv\"\n",
    "    df_rep.to_csv(csv_path, index=False)\n",
    "    mlflow.log_artifact(csv_path, artifact_path=\"tables\")\n",
    "\n",
    "    # 4) confusion matrix -> PNG\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    cm = confusion_matrix(y_true_enc, y_pred_enc, labels=labels_order)\n",
    "    png_path = f\"{file_prefix}_confusion.png\"\n",
    "    _save_cm_png(cm, labels_names, f\"{model_tag} — Confusion Matrix\", png_path)\n",
    "    mlflow.log_artifact(png_path, artifact_path=\"figures\")\n",
    "\n",
    "    return {\"accuracy\": acc, \"macro_f1\": macro_f1, \"cm\": cm, \"report_df\": df_rep}\n"
   ],
   "id": "a37fc0363a524fa6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# --- categorical columns ---\n",
    "CAT_COLS_WIDE = [\"sound_types\", \"plane_tok\", \"side_tok\", \"level_tok\", \"gender\"]\n",
    "NUMERIC_META  = [\"age\"]  # Age \n",
    "KEYS = [\"filename\", \"patient_id\", \"mode\"]\n",
    "\n",
    "# Ensure consistent types/spaces for merge\n",
    "\n",
    "def _norm_cols(df, cols):\n",
    "    df = df.copy()\n",
    "    for c in cols:\n",
    "        if c in df.columns:\n",
    "            df[c] = df[c].astype(str).str.strip()\n",
    "    return df\n",
    "\n",
    "dfc = _norm_cols(df_coherent, [\"filename\",\"patient_id\",\"mode\",\"label\"])\n",
    "dfw = _norm_cols(df_wide.copy(), [\"filename\",\"patient_id\",\"filter_code\"])\n",
    "\n",
    "wide_keep = [\"filename\",\"patient_id\",\"filter_code\",\"diagnoses\",\"label\",\"age\",\"gender\",\n",
    "             \"sound_types\",\"plane_tok\",\"side_tok\",\"level_tok\"]\n",
    "for c in wide_keep:\n",
    "    if c not in dfw.columns:\n",
    "        dfw[c] = np.nan\n",
    "dfw = dfw[wide_keep].rename(columns={\"filter_code\":\"mode\"})\n",
    "\n",
    "# Merge  (filename, patient_id, mode) to adjust df_coherent\n",
    "dfm = pd.merge(\n",
    "    dfc, \n",
    "    dfw, \n",
    "    on=[\"filename\",\"patient_id\",\"mode\"],\n",
    "    how=\"left\",\n",
    "    suffixes=(\"\",\"_wide\")\n",
    ")\n",
    "\n",
    "if \"label\" not in dfm or dfm[\"label\"].isna().all():\n",
    "    if \"label_wide\" in dfm:\n",
    "        dfm[\"label\"] = dfm[\"label_wide\"]\n",
    "    else:\n",
    "        dfm[\"label\"] = dfm[\"diagnoses\"].apply(lambda x: x[0] if isinstance(x, (list, tuple)) and len(x) else \"Unknown\")\n",
    "        \n",
    "dfm.drop(columns=[\"idx\",\"sample_rate\",\"label_wide\"], inplace=True)\n"
   ],
   "id": "6674d5576a0e7962",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# --- Config/inputs ---\n",
    "META = [\"filename\",\"patient_id\",\"label\",\"mode\",\"idx\",\"sample_rate\",\"n_samples\",\"lead_trim_s\",\"fixed_len\"]\n",
    "\n",
    "num_cols = [c for c in dfm.columns if c not in META and pd.api.types.is_numeric_dtype(dfm[c])]\n",
    "\n",
    "# CAT_COLS_WIDE \n",
    "# mode_order: define el orden de modos a concatenar\n",
    "mode_order = sorted(dfm[\"mode\"].dropna().unique().tolist())\n",
    "\n",
    "# --- 1) Agregado NUM por (patient_id, mode) ---\n",
    "agg = (\n",
    "    dfm[[\"patient_id\", \"mode\"] + num_cols]\n",
    "      .groupby([\"patient_id\",\"mode\"], as_index=False)\n",
    "      .mean(numeric_only=True)\n",
    ")\n",
    "\n",
    "# --- 2) Majority label por paciente ---\n",
    "def _majority(s: pd.Series):\n",
    "    s = s.dropna()\n",
    "    if s.empty:\n",
    "        return np.nan\n",
    "    m = s.mode(dropna=True)\n",
    "    return m.iloc[0] if not m.empty else np.nan\n",
    "\n",
    "patient_label = (\n",
    "    dfm.groupby(\"patient_id\", as_index=False)[\"label\"]\n",
    "       .agg(label_majority=_majority)\n",
    ")\n",
    "\n",
    "# --- 3) Majority labels per patient\n",
    "def _majority_strict(s: pd.Series):\n",
    "    s = s.dropna()\n",
    "    s = s[s.astype(str).str.strip() != \"\"]\n",
    "    if s.empty:\n",
    "        return np.nan\n",
    "    m = s.mode()\n",
    "    return m.iloc[0] if len(m) else np.nan\n",
    "\n",
    "if len(CAT_COLS_WIDE) > 0:\n",
    "    cat_majority = (\n",
    "        dfm[[\"patient_id\"] + CAT_COLS_WIDE]\n",
    "        .groupby(\"patient_id\", as_index=False)\n",
    "        .agg({c: _majority_strict for c in CAT_COLS_WIDE})\n",
    "    )\n",
    "else:\n",
    "    cat_majority = pd.DataFrame({\"patient_id\": dfm[\"patient_id\"].dropna().unique()})\n",
    "\n",
    "age_by_patient = (\n",
    "    dfm.groupby(\"patient_id\", as_index=False)[\"age\"]\n",
    "       .mean(numeric_only=True)\n",
    "       .rename(columns={\"age\": \"age_mean\"})\n",
    ")\n",
    "\n",
    "rows = []\n",
    "pids = sorted(dfm[\"patient_id\"].dropna().unique())\n",
    "\n",
    "for pid in pids:\n",
    "    vecs = []\n",
    "    for m in mode_order:\n",
    "        row = agg[(agg[\"patient_id\"] == pid) & (agg[\"mode\"] == m)]\n",
    "        if len(row) == 1:\n",
    "            v = row[num_cols].to_numpy(dtype=float).ravel()\n",
    "        else:\n",
    "            v = np.zeros(len(num_cols), dtype=float)  # padding si falta el modo\n",
    "        vecs.append(v)\n",
    "    concat_vec = np.concatenate(vecs, axis=0)\n",
    "    rows.append({\"patient_id\": pid, \"features\": concat_vec})\n",
    "\n",
    "df_pat = pd.DataFrame(rows)\n",
    "\n",
    "colnames_concat = [f\"{f}__{m}\" for m in mode_order for f in num_cols]\n",
    "num_wide = (\n",
    "    df_pat\n",
    "    .join(pd.DataFrame(df_pat[\"features\"].tolist(), columns=colnames_concat))\n",
    "    .drop(columns=[\"features\"])\n",
    ")\n",
    "\n",
    "# --- 6) Ensemble\n",
    "by_patient = (\n",
    "    num_wide\n",
    "    .merge(patient_label, on=\"patient_id\", how=\"left\")\n",
    "    .merge(age_by_patient, on=\"patient_id\", how=\"left\")\n",
    "    .merge(cat_majority, on=\"patient_id\", how=\"left\")\n",
    ")\n",
    "\n",
    "first_cols = [\"patient_id\", \"label_majority\", \"age_mean\"]\n",
    "cat_cols_present = [c for c in CAT_COLS_WIDE if c in by_patient.columns]\n",
    "other_cols = [c for c in by_patient.columns if c not in set(first_cols + cat_cols_present)]\n",
    "by_patient = by_patient[first_cols + cat_cols_present + other_cols]\n"
   ],
   "id": "ca46b18744b69419",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "One-Hot",
   "id": "f3dc87de77b1a255"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MultiLabelBinarizer, OneHotEncoder\n",
    "\n",
    "CAT_MULTI = [\"sound_types\"]                     # multi-etiqueta\n",
    "CAT_SINGLE = [\"plane_tok\", \"side_tok\", \"level_tok\", \"gender\"]  # una etiqueta\n",
    "\n",
    "X_num_cols = [c for c in by_patient.columns if \"__\" in c]  # columnas pivot por modo\n",
    "X_num = by_patient[X_num_cols].fillna(0.0).astype(float)\n",
    "\n",
    "def to_list_safe(v):\n",
    "    \"\"\"Convierte a lista: NaN->[], string->[string], lista/tuple/ndarray->list(..)\"\"\"\n",
    "    if isinstance(v, (list, tuple, np.ndarray)):\n",
    "        return [str(x).strip() for x in v if pd.notna(x) and str(x).strip() != \"\"]\n",
    "    if pd.isna(v):\n",
    "        return []\n",
    "    return [str(v).strip()]  # si vino como string 'Normal' etc.\n",
    "\n",
    "st_series = by_patient[\"sound_types\"].apply(to_list_safe)\n",
    "mlb = MultiLabelBinarizer()\n",
    "X_st = mlb.fit_transform(st_series)\n",
    "st_names = [f\"st__{c}\" for c in mlb.classes_]  # nombres de columnas multi-hot\n",
    "\n",
    "X_single_raw = by_patient[CAT_SINGLE].astype(object).copy()\n",
    "X_single_raw = X_single_raw.fillna(\"UNK\")  # NaN -> 'UNK'\n",
    "\n",
    "ohe = OneHotEncoder(sparse_output=False, handle_unknown=\"ignore\")\n",
    "X_single = ohe.fit_transform(X_single_raw)\n",
    "single_names = ohe.get_feature_names_out(CAT_SINGLE)\n",
    "\n",
    "X_cat = np.hstack([X_st, X_single])\n",
    "cat_names = list(st_names) + list(single_names)\n",
    "\n",
    "# ===== Etiquetas y grupos =====\n",
    "y = by_patient[\"label_majority\"].astype(str).values\n",
    "groups = by_patient[\"patient_id\"].astype(str).values\n"
   ],
   "id": "1b322e83b70739b3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from numpy import hstack\n",
    "\n",
    "from numpy import hstack\n",
    "X_full = hstack([X_num.values, X_cat])\n",
    "\n",
    "X_train_full, X_test_full, y_train, y_test, \\\n",
    "Xtr_num, Xte_num, Xtr_cat, Xte_cat = train_test_split(\n",
    "    X_full, y, X_num, X_cat, \n",
    "    test_size=0.20, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(\"Train:\", X_train_full.shape, \"Test:\", X_test_full.shape)\n",
    "\n",
    "print(\"Train:\", X_train_full.shape, \"Test:\", X_test_full.shape)\n"
   ],
   "id": "f54a18f18934f2d7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "MrMR",
   "id": "137cc0ac82af2214"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "# --- mRMR-like con scikit-learn: MI - alpha * redundancia (corr absoluta) ---\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "\n",
    "# Parámetros\n",
    "k = min(50, Xtr_num.shape[1])  # nº de features a seleccionar\n",
    "alpha = 0.5                    # peso de penalización por redundancia (ajustable 0–1)\n",
    "\n",
    "# 1) Relevancia: Mutual Information respecto a y\n",
    "mi = mutual_info_classif(Xtr_num, y_train, random_state=42, discrete_features='auto')\n",
    "\n",
    "# 2) Redundancia: matriz de correlación entre features (en train)\n",
    "#    (usamos Pearson |corr| como proxy rápida de redundancia)\n",
    "corr = np.corrcoef(Xtr_num, rowvar=False)  # shape (n_features, n_features)\n",
    "corr = np.nan_to_num(corr, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "corr = np.clip(corr, -1.0, 1.0)\n",
    "abs_corr = np.abs(corr)\n",
    "\n",
    "n_features = Xtr_num.shape[1]\n",
    "remaining = list(range(n_features))\n",
    "selected = []\n",
    "\n",
    "for t in range(min(k, n_features)):\n",
    "    if not selected:\n",
    "        # primera: la de mayor MI\n",
    "        j = int(np.argmax(mi[remaining]))\n",
    "        selected.append(remaining.pop(j))\n",
    "    else:\n",
    "        penalties = []\n",
    "        for idx, feat in enumerate(remaining):\n",
    "            max_red = abs_corr[feat, selected].max() if len(selected) else 0.0\n",
    "            score = mi[feat] - alpha * max_red\n",
    "            penalties.append(score)\n",
    "        j = int(np.argmax(penalties))\n",
    "        selected.append(remaining.pop(j))\n",
    "\n",
    "sel_idx = np.array(selected, dtype=int)\n",
    "print(\"mRMR-like (sklearn) top idx:\", sel_idx[:10])\n",
    "\n",
    "# 3) Aplica selección a train/test\n",
    "# Asegura que sel_idx sean enteros válidos\n",
    "sel_idx = np.asarray(sel_idx, dtype=int)\n",
    "\n",
    "# 1) Nombres de las columnas seleccionadas (desde train)\n",
    "if isinstance(Xtr_num, pd.DataFrame):\n",
    "    # Comprobación de rango\n",
    "    n_cols = Xtr_num.shape[1]\n",
    "    if (sel_idx.min() < 0) or (sel_idx.max() >= n_cols):\n",
    "        raise IndexError(f\"indices fuera de rango: max={sel_idx.max()}, n_cols={n_cols}\")\n",
    "\n",
    "    selected_feat_names = Xtr_num.columns[sel_idx]\n",
    "else:\n",
    "    # Si Xtr_num es array, crea nombres sintéticos\n",
    "    selected_feat_names = np.array([f\"f{i}\" for i in sel_idx])\n",
    "\n",
    "print(\"Features seleccionadas (sample):\", list(selected_feat_names[:10]))\n",
    "\n",
    "# 2) Aplicar la misma selección por NOMBRE a train y test\n",
    "if isinstance(Xtr_num, pd.DataFrame):\n",
    "    Xtr_num_sel = Xtr_num.loc[:, selected_feat_names]\n",
    "else:\n",
    "    Xtr_num_sel = Xtr_num[:, sel_idx]\n",
    "\n",
    "if isinstance(Xte_num, pd.DataFrame):\n",
    "    # Usa los mismos nombres para garantizar alineación\n",
    "    Xte_num_sel = Xte_num.loc[:, selected_feat_names]\n",
    "else:\n",
    "    Xte_num_sel = Xte_num[:, sel_idx]\n"
   ],
   "id": "ba2eb3514e1c4ae",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import umap\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Escalado (UMAP suele ir mejor con features en escala comparable)\n",
    "scaler = StandardScaler()\n",
    "Xtr_scaled = scaler.fit_transform(Xtr_num_sel)\n",
    "Xte_scaled = scaler.transform(Xte_num_sel)\n",
    "\n",
    "# UMAP no supervisado (2D para plot)\n",
    "um = umap.UMAP(\n",
    "    n_neighbors=15,    # estructura local; más alto => más global\n",
    "    min_dist=0.1,      # compacidad de clusters (0.0 = más compacto)\n",
    "    n_components=2,    # 2D para visualizar\n",
    "    metric=\"euclidean\",\n",
    "    random_state=42\n",
    ")\n",
    "Ztr_2d = um.fit_transform(Xtr_scaled)\n",
    "Zte_2d = um.transform(Xte_scaled)\n",
    "\n",
    "# (Opcional) Plot rápido\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set(style=\"white\", rc={\"figure.figsize\": (6,5)})\n",
    "sns.scatterplot(x=Ztr_2d[:,0], y=Ztr_2d[:,1],\n",
    "                hue=y_train, palette=\"tab10\", s=12, alpha=0.9, edgecolor=None)\n",
    "plt.xlabel(\"UMAP-1\"); plt.ylabel(\"UMAP-2\"); plt.title(\"UMAP 2D (train)\")\n",
    "plt.legend(title=\"Clase\", bbox_to_anchor=(1.02, 1), loc=\"upper left\")\n",
    "plt.tight_layout(); plt.show()\n"
   ],
   "id": "d5fc7c8983846c46",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "# =========================\n",
    "# Experimentos KNN (1 y 2)\n",
    "# =========================\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV, LeaveOneOut\n",
    "from sklearn.metrics import (\n",
    "    classification_report, confusion_matrix, ConfusionMatrixDisplay,\n",
    "    f1_score, accuracy_score\n",
    ")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 0) Configuración común\n",
    "# ------------------------------------------------------------\n",
    "# Pipeline: escalado -> KNN\n",
    "pipe = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"clf\", KNeighborsClassifier())\n",
    "])\n",
    "\n",
    "# Hiperparámetros del KNN\n",
    "param_grid = {\n",
    "    \"clf__n_neighbors\": list(range(3, 16)),      # 3..15\n",
    "    \"clf__weights\": [\"uniform\", \"distance\"],\n",
    "    \"clf__p\": [1, 2]                             # 1=Manhattan, 2=Euclídea\n",
    "}\n",
    "\n",
    "# Diagnóstico rápido de y\n",
    "print(\"dtype y:\", getattr(y_train, \"dtype\", type(y_train)))\n",
    "print(\"Ejemplo tipos:\", {type(v) for v in np.asarray(y_train).flatten()})\n",
    "print(\"Clases únicas (primeras 10):\", np.unique(y_train)[:10])\n",
    "\n",
    "# (opcional) Manejo de NaN en etiquetas\n",
    "# y_train = pd.Series(y_train).fillna(\"__MISSING__\")\n",
    "# y_test  = pd.Series(y_test ).fillna(\"__MISSING__\")\n",
    "\n",
    "# Codificación de etiquetas (una sola vez)\n",
    "le = LabelEncoder()\n",
    "y_train_enc = le.fit_transform(y_train)\n",
    "y_test_enc  = le.transform(y_test)\n",
    "\n",
    "# Validador CV: Leave-One-Out (como en el experimento 1 original)\n",
    "loo = LeaveOneOut()\n",
    "\n",
    "from sklearn.metrics import f1_score, accuracy_score, confusion_matrix\n",
    "from sklearn.model_selection import GridSearchCV, LeaveOneOut\n",
    "\n",
    "def run_experiment(name, Xtr, Xte, ytr_enc, yte_enc, label_encoder, plot_cm=True):\n",
    "    grid = GridSearchCV(\n",
    "        estimator=pipe,\n",
    "        param_grid=param_grid,\n",
    "        cv=LeaveOneOut(),\n",
    "        scoring=\"f1_macro\",\n",
    "        n_jobs=-1,\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    # === MLflow run ===\n",
    "    with mlflow.start_run(run_name=f\"KNN | {name}\"):\n",
    "        mlflow.log_param(\"cv\", \"LOO\")\n",
    "        mlflow.log_param(\"search_space_n_neighbors\", \"3..15\")\n",
    "        mlflow.log_param(\"weights\", \"uniform|distance\")\n",
    "        mlflow.log_param(\"p\", \"1|2\")\n",
    "\n",
    "        grid.fit(Xtr, ytr_enc)\n",
    "        best_knn = grid.best_estimator_\n",
    "        y_pred_enc = best_knn.predict(Xte)\n",
    "        y_pred = label_encoder.inverse_transform(y_pred_enc)\n",
    "        y_true = label_encoder.inverse_transform(yte_enc)\n",
    "\n",
    "        # Log de mejores params (json)\n",
    "        mlflow.log_param(\"best_params\", json.dumps(grid.best_params_))\n",
    "        print(f\"\\n[{name}] Mejores hiperparámetros:\", grid.best_params_)\n",
    "        print(f\"[{name}] Mejor puntaje CV (f1_macro): {grid.best_score_:.4f}\")\n",
    "\n",
    "        # Métricas rápidas test\n",
    "        f1_macro_test = f1_score(yte_enc, y_pred_enc, average=\"macro\")\n",
    "        acc_test = accuracy_score(yte_enc, y_pred_enc)\n",
    "\n",
    "        # === Log a MLflow: report + CM (PNG) + métricas ===\n",
    "        aux = mlflow_log_classif(\n",
    "            y_true_str=y_true, y_pred_str=y_pred,\n",
    "            y_true_enc=yte_enc, y_pred_enc=y_pred_enc,\n",
    "            labels_order=np.arange(len(label_encoder.classes_)),\n",
    "            labels_names=label_encoder.classes_,\n",
    "            run_params={\"model\":\"KNN\", \"experiment\": name},\n",
    "            model_tag=\"KNN\",\n",
    "            file_prefix=f\"knn_{'exp1' if 'Full' in name else 'exp2'}\"\n",
    "        )\n",
    "\n",
    "        results = {\n",
    "            \"best_params\": grid.best_params_,\n",
    "            \"cv_f1_macro\": float(grid.best_score_),\n",
    "            \"test_f1_macro\": float(f1_macro_test),\n",
    "            \"test_accuracy\": float(acc_test),\n",
    "            \"classification_report\": aux[\"report_df\"].to_string(index=False),\n",
    "            \"confusion_matrix\": aux[\"cm\"],\n",
    "            \"classes_\": label_encoder.classes_,\n",
    "            \"best_estimator\": best_knn\n",
    "        }\n",
    "        return results\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 1) Experimento 1: Full features (X_train_full / X_test_full)\n",
    "# ------------------------------------------------------------\n",
    "exp1_results = run_experiment(\n",
    "    name=\"Experimento 1 (Full features + LOO)\",\n",
    "    Xtr=X_train_full,\n",
    "    Xte=X_test_full,\n",
    "    ytr_enc=y_train_enc,\n",
    "    yte_enc=y_test_enc,\n",
    "    label_encoder=le,\n",
    "    plot_cm=True\n",
    ")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 2) Experimento 2: mRMR en numéricas + categóricas concatenadas\n",
    "#     Debes tener previamente: Xtr_num_sel, Xtr_cat, Xte_num_sel, Xte_cat\n",
    "# ------------------------------------------------------------\n",
    "X_train_mrmr_cat = np.hstack([Xtr_num_sel, Xtr_cat])\n",
    "X_test_mrmr_cat  = np.hstack([Xte_num_sel, Xte_cat])\n",
    "\n",
    "exp2_results = run_experiment(\n",
    "    name=\"Experimento 2 (mRMR num + cat + LOO)\",\n",
    "    Xtr=X_train_mrmr_cat,\n",
    "    Xte=X_test_mrmr_cat,\n",
    "    ytr_enc=y_train_enc,\n",
    "    yte_enc=y_test_enc,\n",
    "    label_encoder=le,\n",
    "    plot_cm=True\n",
    ")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 3) Key findings al final (resumen compacto)\n",
    "# ------------------------------------------------------------\n",
    "key_findings = {\n",
    "    \"exp1\": {\n",
    "        \"setup\": \"Full features + LOO CV\",\n",
    "        \"best_params\": exp1_results[\"best_params\"],\n",
    "        \"cv_f1_macro\": exp1_results[\"cv_f1_macro\"],\n",
    "        \"test_f1_macro\": exp1_results[\"test_f1_macro\"],\n",
    "        \"test_accuracy\": exp1_results[\"test_accuracy\"]\n",
    "    },\n",
    "    \"exp2\": {\n",
    "        \"setup\": \"mRMR(num) + cat + LOO CV\",\n",
    "        \"best_params\": exp2_results[\"best_params\"],\n",
    "        \"cv_f1_macro\": exp2_results[\"cv_f1_macro\"],\n",
    "        \"test_f1_macro\": exp2_results[\"test_f1_macro\"],\n",
    "        \"test_accuracy\": exp2_results[\"test_accuracy\"]\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"\\n==================== KEY FINDINGS ====================\")\n",
    "for k, v in key_findings.items():\n",
    "    print(f\"\\n{k} -> {v['setup']}\")\n",
    "    print(f\"  best_params  : {v['best_params']}\")\n",
    "    print(f\"  cv_f1_macro  : {v['cv_f1_macro']:.4f}\")\n",
    "    print(f\"  test_f1_macro: {v['test_f1_macro']:.4f}\")\n",
    "    print(f\"  test_accuracy: {v['test_accuracy']:.4f}\")\n",
    "print(\"=====================================================\\n\")\n",
    "\n",
    "# (Opcional) Si quieres comparar rápidamente cuál ganó:\n",
    "winner = max(\n",
    "    [(\"exp1\", key_findings[\"exp1\"][\"test_f1_macro\"]),\n",
    "     (\"exp2\", key_findings[\"exp2\"][\"test_f1_macro\"])],\n",
    "    key=lambda x: x[1]\n",
    ")[0]\n",
    "print(f\"Mejor experimento por F1_macro en test: {winner}\")\n"
   ],
   "id": "14e8353353bdd25a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "# ============================================\n",
    "# Deep Learning Experiments (LSTM / BiGRU / Transformer / TimesNet-like)\n",
    "# mRMR(num) + cat, Stratified 10% validation, Callbacks, CM Heatmap\n",
    "# ============================================\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "\n",
    "# --- Sklearn utils ---\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    classification_report, confusion_matrix, f1_score, accuracy_score\n",
    ")\n",
    "\n",
    "# --- TensorFlow / Keras ---\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, callbacks, optimizers, losses, metrics\n",
    "\n",
    "# Reproducibilidad\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "# ============================================\n",
    "# 0) Data prep (mRMR num + cat) + Label Encoding + Scaling\n",
    "# ============================================\n",
    "X_train_tab = np.hstack([Xtr_num_sel, Xtr_cat])   # (n_train, n_features)\n",
    "X_test_tab  = np.hstack([Xte_num_sel, Xte_cat])   # (n_test,  n_features)\n",
    "\n",
    "# Etiquetas -> enteros\n",
    "le = LabelEncoder()\n",
    "y_train_enc = le.fit_transform(y_train)\n",
    "y_test_enc  = le.transform(y_test)\n",
    "\n",
    "num_classes = len(le.classes_)\n",
    "print(f\"Clases: {list(le.classes_)}\")\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_tab = scaler.fit_transform(X_train_tab)\n",
    "X_test_tab  = scaler.transform(X_test_tab)\n",
    "\n",
    "# ============================================\n",
    "# 1) Stratified Train/Validation split (10%)\n",
    "# ============================================\n",
    "Xtr_tab, Xval_tab, ytr_enc, yval_enc = train_test_split(\n",
    "    X_train_tab, y_train_enc, test_size=0.10, random_state=SEED, stratify=y_train_enc\n",
    ")\n",
    "\n",
    "# ============================================\n",
    "# 2) Dar forma 3D para redes secuenciales\n",
    "# ============================================\n",
    "def to_3d(x2d):\n",
    "    # (N, F) -> (N, T=1, F)\n",
    "    return np.expand_dims(x2d, axis=1)\n",
    "\n",
    "Xtr = to_3d(Xtr_tab)\n",
    "Xval = to_3d(Xval_tab)\n",
    "Xte  = to_3d(X_test_tab)\n",
    "\n",
    "input_shape = Xtr.shape[1:]  # (T, F)\n",
    "\n",
    "# ============================================\n",
    "# 3) Model builders\n",
    "# ============================================\n",
    "def build_lstm(input_shape, num_classes, hidden=64, dropout=0.2):\n",
    "    inp = layers.Input(shape=input_shape)\n",
    "    x = layers.Masking()(inp)\n",
    "    x = layers.LSTM(hidden, return_sequences=False)(x)\n",
    "    x = layers.Dropout(dropout)(x)\n",
    "    x = layers.Dense(64, activation=\"relu\")(x)\n",
    "    out = layers.Dense(num_classes, activation=\"softmax\")(x)\n",
    "    return models.Model(inp, out, name=\"LSTM\")\n",
    "\n",
    "def build_bigru(input_shape, num_classes, hidden=64, dropout=0.2):\n",
    "    inp = layers.Input(shape=input_shape)\n",
    "    x = layers.Masking()(inp)\n",
    "    x = layers.Bidirectional(layers.GRU(hidden, return_sequences=False))(x)\n",
    "    x = layers.Dropout(dropout)(x)\n",
    "    x = layers.Dense(64, activation=\"relu\")(x)\n",
    "    out = layers.Dense(num_classes, activation=\"softmax\")(x)\n",
    "    return models.Model(inp, out, name=\"BiGRU\")\n",
    "\n",
    "def build_transformer(input_shape, num_classes, d_model=64, num_heads=4, ff_dim=128, dropout=0.2):\n",
    "    inp = layers.Input(shape=input_shape)              # (T, F)\n",
    "    x = layers.Dense(d_model)(inp)                     # (T, d_model)\n",
    "\n",
    "    # Positional encoding aprendible\n",
    "    T = tf.shape(x)[1]\n",
    "    pos = tf.range(start=0, limit=T, delta=1)\n",
    "    pos_emb = layers.Embedding(input_dim=2048, output_dim=d_model)(pos)  # 2048 por si T varía\n",
    "    pos_emb = pos_emb[tf.newaxis, ...]                # (1, T, d_model)\n",
    "    x = x + pos_emb                                   # broadcast a (B, T, d_model)\n",
    "\n",
    "    for _ in range(2):\n",
    "        attn_out = layers.MultiHeadAttention(num_heads=num_heads, key_dim=d_model)(x, x)\n",
    "        x = layers.LayerNormalization(epsilon=1e-6)(x + attn_out)\n",
    "\n",
    "        # ⬇️ Cambio clave: usar models.Sequential, no layers.Sequential\n",
    "        ffn = models.Sequential([\n",
    "            layers.Dense(ff_dim, activation=\"relu\"),\n",
    "            layers.Dense(d_model)\n",
    "        ])\n",
    "        x = layers.LayerNormalization(epsilon=1e-6)(x + ffn(x))\n",
    "\n",
    "    x = layers.GlobalAveragePooling1D()(x)\n",
    "    x = layers.Dropout(dropout)(x)\n",
    "    x = layers.Dense(64, activation=\"relu\")(x)\n",
    "    out = layers.Dense(num_classes, activation=\"softmax\")(x)\n",
    "    return models.Model(inp, out, name=\"Transformer\")\n",
    "\n",
    "def build_timesnet_like(input_shape, num_classes, channels=64, dropout=0.2):\n",
    "    \"\"\"\n",
    "    Bloque inspirado en TimesNet: multi-period 1D convs con diferentes kernels/dilataciones\n",
    "    \"\"\"\n",
    "    inp = layers.Input(shape=input_shape)   # (T, F)\n",
    "\n",
    "    # Proyección a canales\n",
    "    x0 = layers.Dense(channels, activation=\"relu\")(inp)  # (T, C)\n",
    "\n",
    "    # Multi-period conv blocks\n",
    "    ks = [1, 3, 5]            # kernels\n",
    "    dil = [1, 2, 4]           # dilations\n",
    "    feats = []\n",
    "    for k in ks:\n",
    "        for d in dil:\n",
    "            y = layers.Conv1D(channels, kernel_size=k, dilation_rate=d, padding=\"causal\", activation=\"relu\")(x0)\n",
    "            y = layers.BatchNormalization()(y)\n",
    "            y = layers.Conv1D(channels, kernel_size=1, padding=\"same\", activation=\"relu\")(y)\n",
    "            feats.append(layers.GlobalAveragePooling1D()(y))\n",
    "\n",
    "    x = layers.Concatenate()(feats) if len(feats) > 1 else feats[0]\n",
    "    x = layers.Dropout(dropout)(x)\n",
    "    x = layers.Dense(128, activation=\"relu\")(x)\n",
    "    out = layers.Dense(num_classes, activation=\"softmax\")(x)\n",
    "    return models.Model(inp, out, name=\"TimesNetLike\")\n",
    "\n",
    "MODEL_BUILDERS = {\n",
    "    \"LSTM\": build_lstm,\n",
    "    \"BiGRU\": build_bigru,\n",
    "    \"Transformer\": build_transformer,\n",
    "    \"TimesNet\": build_timesnet_like\n",
    "}\n",
    "\n",
    "# ============================================\n",
    "# 4) Entrenamiento con callbacks y evaluación\n",
    "# ============================================\n",
    "def train_and_eval(model_name, Xtr, ytr, Xval, yval, Xte, yte, input_shape, num_classes, base_dir=\"nn_runs\"):\n",
    "    import os, numpy as np\n",
    "    os.makedirs(base_dir, exist_ok=True)\n",
    "\n",
    "    with mlflow.start_run(run_name=f\"DL | {model_name}\"):\n",
    "        mlflow.log_param(\"model\", model_name)\n",
    "        mlflow.log_param(\"optimizer\", \"adam\")\n",
    "        mlflow.log_param(\"batch_size\", 64)\n",
    "        mlflow.log_param(\"epochs_max\", 200)\n",
    "\n",
    "        model = MODEL_BUILDERS[model_name](input_shape, num_classes)\n",
    "        model.compile(\n",
    "            optimizer=optimizers.Adam(learning_rate=1e-3),\n",
    "            loss=losses.SparseCategoricalCrossentropy(),\n",
    "            metrics=[metrics.SparseCategoricalAccuracy(name=\"acc\")]\n",
    "        )\n",
    "\n",
    "        # --- checkpoint para guardar el mejor peso ---\n",
    "        ckpt_path = os.path.join(base_dir, f\"best_{model_name}.weights.h5\")\n",
    "        cbs = [\n",
    "            callbacks.EarlyStopping(monitor=\"val_loss\", patience=10, restore_best_weights=True, verbose=1),\n",
    "            callbacks.ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=5, min_lr=1e-5, verbose=1),\n",
    "            callbacks.ModelCheckpoint(ckpt_path, monitor=\"val_loss\",\n",
    "                                      save_best_only=True, save_weights_only=True, verbose=1)\n",
    "        ]\n",
    "\n",
    "        history = model.fit(\n",
    "            Xtr, ytr,\n",
    "            validation_data=(Xval, yval),\n",
    "            epochs=200, batch_size=64,\n",
    "            callbacks=cbs, verbose=1\n",
    "        )\n",
    "\n",
    "        best_ckpt_exists = os.path.exists(ckpt_path)\n",
    "        if best_ckpt_exists:\n",
    "            try:\n",
    "                model.load_weights(ckpt_path)\n",
    "            except Exception as e:\n",
    "                print(f\"[WARN] No pude cargar {ckpt_path}: {e}\")\n",
    "\n",
    "        # log de validación\n",
    "        mlflow.log_metric(\"val_loss_min\", float(np.min(history.history[\"val_loss\"])))\n",
    "        if \"val_acc\" in history.history:\n",
    "            mlflow.log_metric(\"val_acc_max\", float(np.max(history.history[\"val_acc\"])))\n",
    "        elif \"val_sparse_categorical_accuracy\" in history.history:\n",
    "            mlflow.log_metric(\"val_acc_max\", float(np.max(history.history[\"val_sparse_categorical_accuracy\"])))\n",
    "\n",
    "        # evaluación test\n",
    "        y_pred_proba = model.predict(Xte, verbose=0)\n",
    "        y_pred_enc = np.argmax(y_pred_proba, axis=1)\n",
    "\n",
    "        test_acc  = accuracy_score(yte, y_pred_enc)\n",
    "        test_f1   = f1_score(yte, y_pred_enc, average=\"macro\")\n",
    "        print(f\"\\n[{model_name}] Test accuracy: {test_acc:.4f} | Test F1(macro): {test_f1:.4f}\\n\")\n",
    "\n",
    "        # log MLflow: reporte + matriz de confusión\n",
    "        y_true_str = le.inverse_transform(yte)\n",
    "        y_pred_str = le.inverse_transform(y_pred_enc)\n",
    "        aux = mlflow_log_classif(\n",
    "            y_true_str=y_true_str, y_pred_str=y_pred_str,\n",
    "            y_true_enc=yte, y_pred_enc=y_pred_enc,\n",
    "            labels_order=np.arange(num_classes),\n",
    "            labels_names=le.classes_,\n",
    "            run_params={\"model\": model_name},\n",
    "            model_tag=model_name,\n",
    "            file_prefix=f\"dl_{model_name.lower()}\"\n",
    "        )\n",
    "\n",
    "        if best_ckpt_exists:\n",
    "            try:\n",
    "                mlflow.log_artifact(ckpt_path, artifact_path=\"checkpoints\")\n",
    "            except Exception as e:\n",
    "                print(f\"[INFO] No subí el checkpoint a MLflow: {e}\")\n",
    "\n",
    "        return {\n",
    "            \"model_name\": model_name,\n",
    "            \"best_weights_path\": ckpt_path if best_ckpt_exists else None,\n",
    "            \"val_loss_min\": float(np.min(history.history[\"val_loss\"])),\n",
    "            \"val_acc_max\": float(np.max(history.history.get(\"val_acc\",\n",
    "                                     history.history.get(\"val_sparse_categorical_accuracy\", [np.nan])))),\n",
    "            \"test_accuracy\": float(test_acc),\n",
    "            \"test_f1_macro\": float(test_f1),\n",
    "            \"classification_report\": aux[\"report_df\"].to_string(index=False),\n",
    "            \"confusion_matrix\": aux[\"cm\"]\n",
    "        }\n",
    "\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# 5) Correr los 4 modelos\n",
    "# ============================================\n",
    "key_findings = {}\n",
    "for name in [\"LSTM\", \"BiGRU\", \"Transformer\", \"TimesNet\"]:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"Entrenando: {name}\")\n",
    "    print(\"=\"*80)\n",
    "    res = train_and_eval(\n",
    "        model_name=name,\n",
    "        Xtr=Xtr, ytr=ytr_enc,\n",
    "        Xval=Xval, yval=yval_enc,\n",
    "        Xte=Xte,  yte=y_test_enc,\n",
    "        input_shape=input_shape,\n",
    "        num_classes=num_classes,\n",
    "        base_dir=\"nn_runs\"\n",
    "    )\n",
    "    key_findings[name] = {\n",
    "        \"val_loss_min\": res[\"val_loss_min\"],\n",
    "        \"val_acc_max\": res[\"val_acc_max\"],\n",
    "        \"test_accuracy\": res[\"test_accuracy\"],\n",
    "        \"test_f1_macro\": res[\"test_f1_macro\"],\n",
    "        \"best_ckpt\": res[\"best_weights_path\"]\n",
    "    }\n",
    "\n",
    "# ============================================\n",
    "# 6) Resumen final (Key Findings)\n",
    "# ============================================\n",
    "print(\"\\n==================== KEY FINDINGS (DL) ====================\")\n",
    "for k, v in key_findings.items():\n",
    "    print(f\"\\n{k}\")\n",
    "    print(f\"  val_loss_min : {v['val_loss_min']:.4f}\")\n",
    "    print(f\"  val_acc_max  : {v['val_acc_max'] if v['val_acc_max'] is not None else 'N/A'}\")\n",
    "    print(f\"  test_f1_macro: {v['test_f1_macro']:.4f}\")\n",
    "    print(f\"  test_accuracy: {v['test_accuracy']:.4f}\")\n",
    "    print(f\"  best_ckpt    : {v['best_ckpt']}\")\n",
    "print(\"===========================================================\\n\")\n",
    "\n",
    "# (Opcional) Modelo ganador por F1 en test\n",
    "winner = max([(k, v[\"test_f1_macro\"]) for k, v in key_findings.items()], key=lambda x: x[1])[0]\n",
    "print(f\"Mejor modelo (F1_macro test): {winner}\")\n"
   ],
   "id": "a136f49df36f3e9c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "# =========================\n",
    "# 3.c - RandomForest \n",
    "# =========================\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "from sklearn.metrics import (\n",
    "    classification_report, confusion_matrix, ConfusionMatrixDisplay,\n",
    "    f1_score, accuracy_score\n",
    ")\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "# ==========================================================\n",
    "# 0) Etiquetas (reutiliza y_train/y_test existentes)\n",
    "# ==========================================================\n",
    "le_rf = LabelEncoder()\n",
    "y_train_enc_rf = le_rf.fit_transform(y_train)\n",
    "y_test_enc_rf  = le_rf.transform(y_test)\n",
    "\n",
    "# CV estratificado (LOO en RF suele ser costoso y poco necesario)\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# ==========================================================\n",
    "# 1) Definición del modelo y grid\n",
    "\n",
    "# ==========================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_jobs=-1,\n",
    "    random_state=42,\n",
    "    oob_score=True,         # requiere bootstrap=True\n",
    ")\n",
    "\n",
    "param_grid_rf = {\n",
    "    \"clf__n_estimators\": [50, 100, 200],\n",
    "    \"clf__max_depth\": [None, 12, 18],\n",
    "    \"clf__min_samples_split\": [2, 5],\n",
    "    \"clf__min_samples_leaf\": [1, 2, 4],\n",
    "    \"clf__max_features\": [\"sqrt\", \"log2\", 0.5],\n",
    "    \"clf__bootstrap\": [True],                       # necesario para OOB\n",
    "    \"clf__class_weight\": [None, \"balanced_subsample\"]\n",
    "}\n",
    "\n",
    "def run_rf_experiment(name, Xtr, Xte, ytr_enc, yte_enc, label_encoder,\n",
    "                      plot_cm=True, do_perm_importance=True, feature_names=None):\n",
    "    pipe = Pipeline([(\"clf\", rf)])\n",
    "    grid = GridSearchCV(\n",
    "        estimator=pipe, param_grid=param_grid_rf,\n",
    "        cv=cv, scoring=\"f1_macro\", n_jobs=-1, verbose=1\n",
    "    )\n",
    "\n",
    "    with mlflow.start_run(run_name=f\"RF | {name}\"):\n",
    "        mlflow.log_param(\"cv\", \"StratifiedKFold(5)\")\n",
    "        mlflow.log_param(\"oob_score\", True)\n",
    "\n",
    "        grid.fit(Xtr, ytr_enc)\n",
    "        best_model = grid.best_estimator_\n",
    "        rf_inner = best_model.named_steps[\"clf\"]\n",
    "\n",
    "        mlflow.log_param(\"best_params\", json.dumps(grid.best_params_))\n",
    "        print(f\"\\n[{name}] Mejores hiperparámetros:\", grid.best_params_)\n",
    "        print(f\"[{name}] Mejor puntaje CV (f1_macro): {grid.best_score_:.4f}\")\n",
    "\n",
    "        # Pred test\n",
    "        y_pred_enc = best_model.predict(Xte)\n",
    "        y_pred = label_encoder.inverse_transform(y_pred_enc)\n",
    "        y_true = label_encoder.inverse_transform(yte_enc)\n",
    "\n",
    "        # === Log a MLflow: report + CM + métricas ===\n",
    "        aux = mlflow_log_classif(\n",
    "            y_true_str=y_true, y_pred_str=y_pred,\n",
    "            y_true_enc=yte_enc, y_pred_enc=y_pred_enc,\n",
    "            labels_order=np.arange(len(label_encoder.classes_)),\n",
    "            labels_names=label_encoder.classes_,\n",
    "            run_params={\"model\":\"RandomForest\", \"experiment\": name},\n",
    "            model_tag=\"RF\",\n",
    "            file_prefix=f\"rf_{'full' if 'Full' in name else 'mrmr'}\"\n",
    "        )\n",
    "\n",
    "        # OOB\n",
    "        oob = getattr(rf_inner, \"oob_score_\", None)\n",
    "        if oob is not None:\n",
    "            mlflow.log_metric(\"oob_score\", float(oob))\n",
    "\n",
    "        # (Opcional) Permutation Importance -> guarda PNG y súbelo\n",
    "        if do_perm_importance:\n",
    "            from sklearn.inspection import permutation_importance\n",
    "            r = permutation_importance(best_model, Xte, yte_enc, n_repeats=10, random_state=42, n_jobs=-1, scoring=\"f1_macro\")\n",
    "            top_k = min(20, r.importances_mean.shape[0])\n",
    "            idx = np.argsort(r.importances_mean)[::-1][:top_k]\n",
    "            names = [f\"feat_{i}\" for i in idx] if feature_names is None else [feature_names[i] for i in idx]\n",
    "            plt.figure(figsize=(8, 6))\n",
    "            plt.barh(range(top_k), r.importances_mean[idx][::-1], xerr=r.importances_std[idx][::-1])\n",
    "            plt.yticks(range(top_k), names[::-1])\n",
    "            plt.xlabel(\"Permutation importance (Δf1_macro)\")\n",
    "            plt.title(f\"{name} - Top-{top_k} features\")\n",
    "            plt.tight_layout()\n",
    "            pi_path = f\"rf_perm_importance_{'full' if 'Full' in name else 'mrmr'}.png\"\n",
    "            plt.savefig(pi_path, dpi=160, bbox_inches=\"tight\"); plt.close()\n",
    "            mlflow.log_artifact(pi_path, artifact_path=\"figures\")\n",
    "\n",
    "        return {\n",
    "            \"best_params\": grid.best_params_,\n",
    "            \"cv_f1_macro\": float(grid.best_score_),\n",
    "            \"test_f1_macro\": float(aux['macro_f1']),\n",
    "            \"test_accuracy\": float(aux['accuracy']),\n",
    "            \"classification_report\": aux[\"report_df\"].to_string(index=False),\n",
    "            \"confusion_matrix\": aux[\"cm\"],\n",
    "            \"classes_\": label_encoder.classes_,\n",
    "            \"best_estimator\": best_model,\n",
    "            \"oob_score\": float(oob) if oob is not None else None\n",
    "        }\n",
    "\n",
    "\n",
    "# ==========================================================\n",
    "# 2) Conjuntos de features (paralelo a tus Exp 1 y 2)\n",
    "# ==========================================================\n",
    "feature_sets = [\n",
    "    (\"RF - Full features\", X_train_full, X_test_full),\n",
    "    (\"RF - mRMR(num) + cat\", np.hstack([Xtr_num_sel, Xtr_cat]),\n",
    "                             np.hstack([Xte_num_sel, Xte_cat])),\n",
    "]\n",
    "\n",
    "feature_names_full = None      # lista de strings o None\n",
    "feature_names_mrmr = None      # idem\n",
    "\n",
    "# ==========================================================\n",
    "# 3) Ejecutar y recolectar resultados\n",
    "# ==========================================================\n",
    "rf_key_findings = {}\n",
    "for fs_name, Xtr, Xte in feature_sets:\n",
    "    feats_names = feature_names_full if \"Full\" in fs_name else feature_names_mrmr\n",
    "    res = run_rf_experiment(\n",
    "        name=fs_name,\n",
    "        Xtr=Xtr, Xte=Xte,\n",
    "        ytr_enc=y_train_enc_rf, yte_enc=y_test_enc_rf,\n",
    "        label_encoder=le_rf,\n",
    "        plot_cm=True,\n",
    "        do_perm_importance=True,\n",
    "        feature_names=feats_names\n",
    "    )\n",
    "    rf_key_findings[fs_name] = {\n",
    "        \"setup\": f\"{fs_name} + RandomForest + 5-fold CV + OOB\",\n",
    "        \"best_params\": res[\"best_params\"],\n",
    "        \"cv_f1_macro\": res[\"cv_f1_macro\"],\n",
    "        \"test_f1_macro\": res[\"test_f1_macro\"],\n",
    "        \"test_accuracy\": res[\"test_accuracy\"],\n",
    "        \"oob_score\": res[\"oob_score\"]\n",
    "    }\n",
    "\n",
    "# ==========================================================\n",
    "# 4) Resumen final: KEY FINDINGS RF\n",
    "# ==========================================================\n",
    "print(\"\\n==================== KEY FINDINGS (RandomForest) ====================\")\n",
    "for k, v in rf_key_findings.items():\n",
    "    print(f\"\\n{k} -> {v['setup']}\")\n",
    "    print(f\"  best_params  : {v['best_params']}\")\n",
    "    print(f\"  cv_f1_macro  : {v['cv_f1_macro']:.4f}\")\n",
    "    print(f\"  test_f1_macro: {v['test_f1_macro']:.4f}\")\n",
    "    print(f\"  test_accuracy: {v['test_accuracy']:.4f}\")\n",
    "    if v[\"oob_score\"] is not None:\n",
    "        print(f\"  oob_score    : {v['oob_score']:.4f}\")\n",
    "print(\"=====================================================================\\n\")\n",
    "\n",
    "# (Opcional) Ganador por F1_macro en test\n",
    "winner = max(\n",
    "    [(k, v[\"test_f1_macro\"]) for k, v in rf_key_findings.items()],\n",
    "    key=lambda x: x[1]\n",
    ")[0]\n",
    "print(f\"Mejor configuración RF por F1_macro en test: {winner}\")\n"
   ],
   "id": "83b3461848a13408",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##Key Findings ##",
   "id": "9d7c5c01b690147e"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "import mlflow, pandas as pd, os, glob\n",
    "from IPython.display import display, Markdown, Image\n",
    "from mlflow import artifacts as mlart\n",
    "\n",
    "exp_name = \"samay-audio\"\n",
    "runs = mlflow.search_runs(experiment_names=[exp_name])\n",
    "\n",
    "display(Markdown(\"# Key Findings\"))\n",
    "\n",
    "# 5.1 Tabla resumen (ordenada por macro_f1)\n",
    "cols = [\"run_id\",\"run_name\",\"params.model\",\"params.best_params\",\"metrics.accuracy\",\"metrics.macro_f1\",\"metrics.oob_score\",\"start_time\"]\n",
    "present = [c for c in cols if c in runs.columns]\n",
    "df_summary = runs[present].copy()\n",
    "if \"metrics.macro_f1\" in df_summary:\n",
    "    df_summary = df_summary.sort_values(\"metrics.macro_f1\", ascending=False)\n",
    "display(Markdown(\"## Resumen de runs\"))\n",
    "display(df_summary)\n",
    "\n",
    "if \"params.model\" in runs.columns:\n",
    "    display(Markdown(\"## Mejores resultados por modelo\"))\n",
    "    best_by_model = (runs\n",
    "        .dropna(subset=[\"metrics.macro_f1\", \"params.model\"])\n",
    "        .sort_values(\"metrics.macro_f1\", ascending=False)\n",
    "        .groupby(\"params.model\")\n",
    "        .head(1)\n",
    "    )\n",
    "\n"
   ],
   "id": "fcbdb7856df37ce0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "# ===== Key Findings robusto (con matrices de confusión) =====\n",
    "import mlflow, os, glob\n",
    "import pandas as pd\n",
    "from IPython.display import display, Markdown, Image\n",
    "from mlflow import artifacts as mlart\n",
    "\n",
    "EXP_NAME = \"samay-audio\"   # el mismo que usaste en set_experiment\n",
    "runs = mlflow.search_runs(experiment_names=[EXP_NAME])\n",
    "\n",
    "display(Markdown(\"# Key Findings\"))\n",
    "\n",
    "# -------- 1) Resumen de runs (columna correcta de nombre) --------\n",
    "# Normaliza columnas disponibles\n",
    "name_col = \"tags.mlflow.runName\" if \"tags.mlflow.runName\" in runs.columns else None\n",
    "model_col = \"params.model\" if \"params.model\" in runs.columns else None\n",
    "\n",
    "cols_wanted = [\"run_id\",\n",
    "               name_col if name_col else None,\n",
    "               model_col if model_col else None,\n",
    "               \"params.best_params\" if \"params.best_params\" in runs.columns else None,\n",
    "               \"metrics.accuracy\" if \"metrics.accuracy\" in runs.columns else None,\n",
    "               \"metrics.macro_f1\" if \"metrics.macro_f1\" in runs.columns else None,\n",
    "               \"metrics.oob_score\" if \"metrics.oob_score\" in runs.columns else None,\n",
    "               \"start_time\" if \"start_time\" in runs.columns else None]\n",
    "cols_wanted = [c for c in cols_wanted if c is not None and c in runs.columns]\n",
    "\n",
    "df_sum = runs[cols_wanted].copy()\n",
    "if \"metrics.macro_f1\" in df_sum.columns:\n",
    "    df_sum = df_sum.sort_values(\"metrics.macro_f1\", ascending=False)\n",
    "\n",
    "# Renombra columnas para que se vean bonito\n",
    "renames = {}\n",
    "if name_col: renames[name_col] = \"run_name\"\n",
    "if model_col: renames[model_col] = \"model\"\n",
    "df_sum = df_sum.rename(columns=renames)\n",
    "\n",
    "display(Markdown(\"## Resumen de runs\"))\n",
    "display(df_sum)\n",
    "\n",
    "# -------- 2) Mejor por modelo (si existe params.model) --------\n",
    "if model_col and \"metrics.macro_f1\" in runs.columns:\n",
    "    display(Markdown(\"## Mejores resultados por modelo\"))\n",
    "    best_by_model = (runs\n",
    "        .dropna(subset=[\"metrics.macro_f1\", model_col])\n",
    "        .sort_values(\"metrics.macro_f1\", ascending=False)\n",
    "        .groupby(model_col)\n",
    "        .head(1)\n",
    "    )\n",
    "    bbm_show = best_by_model[[\"run_id\",\n",
    "                              name_col] if name_col else [\"run_id\"]].copy()\n",
    "    if name_col: bbm_show = bbm_show.rename(columns={name_col: \"run_name\"})\n",
    "    # añade métricas básicas si existen\n",
    "    for c in [\"metrics.accuracy\", \"metrics.macro_f1\"]:\n",
    "        if c in best_by_model.columns:\n",
    "            bbm_show[c] = best_by_model[c].values\n",
    "    display(bbm_show)\n",
    "\n",
    "# -------- 3) Galería: matrices de confusión (y demás PNGs) --------\n",
    "display(Markdown(\"## Matrices de confusión y figuras\"))\n",
    "any_fig = False\n",
    "\n",
    "for _, row in runs.iterrows():\n",
    "    run_id = row[\"run_id\"]\n",
    "    run_name = row.get(name_col, None) if name_col else None\n",
    "    if not run_name:\n",
    "        # Fallback: usa el run_id corto como nombre\n",
    "        run_name = f\"run_{run_id[:8]}\"\n",
    "\n",
    "    # Intenta descargar la carpeta \"figures\" del run\n",
    "    try:\n",
    "        out_dir = mlart.download_artifacts(artifact_uri=f\"runs:/{run_id}/figures\")\n",
    "    except Exception:\n",
    "        # si el run no tiene carpeta 'figures', saltamos\n",
    "        continue\n",
    "\n",
    "    png_conf = sorted(glob.glob(os.path.join(out_dir, \"*confusion*.png\")))\n",
    "    png_all  = sorted(glob.glob(os.path.join(out_dir, \"*.png\")))\n",
    "\n",
    "    to_show = png_conf if png_conf else png_all\n",
    "    if not to_show:\n",
    "        continue\n",
    "\n",
    "    any_fig = True\n",
    "    display(Markdown(f\"**{run_name}**  \\n_run_id:_ `{run_id}`\"))\n",
    "    for p in to_show:\n",
    "        display(Image(filename=p, width=580))\n",
    "\n",
    "if not any_fig:\n",
    "    display(Markdown(\"> No se encontraron PNGs en `figures/` de los runs. \"\n",
    "                     \"Verifica que se estén logueando como `mlflow.log_artifact(<png>, artifact_path='figures')`.\"))\n"
   ],
   "id": "7b9f22a9bba32f8f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "25ca35ff9fde1361",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (SAMAY)",
   "language": "python",
   "name": "samay"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
